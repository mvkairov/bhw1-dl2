{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "37b38cce",
   "metadata": {
    "cellId": "tybx6jf49lk9gf4kkq62"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "be46e15b",
   "metadata": {
    "cellId": "8g7dikaaffkfhuth2i7dat"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/jupyter/.local/lib/python3.10/site-packages (0.1.99)\n",
      "Requirement already satisfied: wandb in /home/jupyter/.local/lib/python3.10/site-packages (0.16.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.6)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.32)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /kernel/lib/python3.10/site-packages (from wandb) (5.7.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from wandb) (1.38.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/jupyter/.local/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in /home/jupyter/.local/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /kernel/lib/python3.10/site-packages (from wandb) (51.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /kernel/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Collecting charset-normalizer~=2.0.0 (from requests<3,>=2.0.0->wandb)\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /kernel/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Installing collected packages: charset-normalizer\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed charset-normalizer-2.0.12\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "%pip install sentencepiece wandb tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "509538e9",
   "metadata": {
    "cellId": "mat8bhd6qygcull3dh3g7o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mLLAMA\u001b[0m/\n",
      "bpe.model\n",
      "bpe.vocab\n",
      "checkpoint.ipynb\n",
      "checkpoint.pth\n",
      "\u001b[01;34mdata\u001b[0m/\n",
      "readme.md\n",
      "\u001b[01;34mwandb\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d136391d",
   "metadata": {
    "cellId": "t6w5q4yt8odf7ngukz88l"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "import sys\n",
    "sys.path.append('LLAMA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8c189235",
   "metadata": {
    "cellId": "r39ncyjujah7v7z88to7uc"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "from LLAMA.tinystories import TinyStoriesDataset\n",
    "from LLAMA.model import LLAMA\n",
    "from LLAMA.train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a5c50518",
   "metadata": {
    "cellId": "j2vjjz3wehk07lzmx6jvh1t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer ok\n",
      "texts ok\n",
      "encoding ok\n",
      "tokenizer ok\n",
      "texts ok\n",
      "encoding ok\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "train_ds = TinyStoriesDataset(data_file=\"data/train.txt\", sp_model_prefix=\"bpe\")\n",
    "val_ds = TinyStoriesDataset(data_file=\"data/val.txt\", sp_model_prefix=\"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc1a4794",
   "metadata": {
    "cellId": "cg0g8qbxsmo8h9p4fwy3oy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2717700 27631 15000\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "from torch.utils.data import DataLoader\n",
    "train_load = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_load = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "print(len(train_ds), len(val_ds), train_ds.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "28c54635",
   "metadata": {
    "cellId": "glo8f2tv4t411z5sxwgl2uh"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "import wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "035dd5eb",
   "metadata": {
    "cellId": "mxl9c845jt9429yeplt4w"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jupyter/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmkairov\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/jupyter/work/resources/bhwdl/wandb/run-20231204_005425-g4s2wva9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrun_min3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/mkairov/bhwdl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mkairov/bhwdl/runs/g4s2wva9\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mkairov/bhwdl/runs/g4s2wva9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f0fb89c47f0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "wandb.login(key='67d3c077c0b4327f7734330abc8ede91d268977f', relogin=True)\n",
    "wandb.init('bhwdl', name='run_min3', reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b1a38",
   "metadata": {
    "cellId": "3fiwnzs60z9cxozl73wfus"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "!git pull https://github.com/mvkairov/bhwdl.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "56d1094f",
   "metadata": {
    "cellId": "q0noqdq486qynvdhe1z7cg"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "model = LLAMA(8, 512, 8, train_ds.vocab_size, 768, 0.1)\n",
    "model = model.to('cuda')\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
    "scheduler = OneCycleLR(optimizer=optimizer, max_lr=3e-4, epochs=32, steps_per_epoch=1500, pct_start=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1d6e7bc8",
   "metadata": {
    "cellId": "a4ij9u0k1jbves77eipmj"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f239081cd3604f3e8721e40a27f95181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 0.645\n",
      "epoch: 0, train loss: 9.173\n",
      "epoch: 0, train loss: 8.542\n",
      "epoch: 0, train loss: 8.105\n",
      "epoch: 0, train loss: 7.775\n",
      "epoch: 0, train loss: 7.505\n",
      "epoch: 0, train loss: 7.259\n",
      "epoch: 0, train loss: 7.036\n",
      "epoch: 0, train loss: 6.813\n",
      "epoch: 0, train loss: 6.599\n",
      "epoch: 0, train loss: 6.395\n",
      "epoch: 0, train loss: 6.229\n",
      "epoch: 0, train loss: 6.077\n",
      "epoch: 0, train loss: 5.923\n",
      "epoch: 0, train loss: 5.824\n",
      "epoch: 0, train loss: 5.700\n",
      "epoch: 0, train loss: 5.614\n",
      "epoch: 0, train loss: 5.537\n",
      "epoch: 0, train loss: 5.429\n",
      "epoch: 0, train loss: 5.343\n",
      "epoch: 0, train loss: 5.281\n",
      "epoch: 0, train loss: 5.238\n",
      "epoch: 0, train loss: 5.212\n",
      "epoch: 0, train loss: 5.120\n",
      "epoch: 0, train loss: 5.117\n",
      "epoch: 0, train loss: 5.052\n",
      "epoch: 0, train loss: 5.016\n",
      "epoch: 0, train loss: 4.974\n",
      "epoch: 0, train loss: 4.959\n",
      "epoch: 0, train loss: 4.877\n",
      "epoch: 0, train loss: 4.844\n",
      "epoch: 0, train loss: 4.835\n",
      "epoch: 0, train loss: 4.777\n",
      "epoch: 0, train loss: 4.752\n",
      "epoch: 0, train loss: 4.746\n",
      "epoch: 0, train loss: 4.718\n",
      "epoch: 0, train loss: 4.647\n",
      "epoch: 0, train loss: 4.654\n",
      "epoch: 0, train loss: 4.629\n",
      "epoch: 0, train loss: 4.598\n",
      "epoch: 0, train loss: 4.525\n",
      "epoch: 0, train loss: 4.522\n",
      "epoch: 0, train loss: 4.533\n",
      "epoch: 0, train loss: 4.490\n",
      "epoch: 0, train loss: 4.476\n",
      "epoch: 0, train loss: 4.458\n",
      "epoch: 0, train loss: 4.427\n",
      "epoch: 0, train loss: 4.414\n",
      "epoch: 0, train loss: 4.380\n",
      "epoch: 0, train loss: 4.398\n",
      "epoch: 0, train loss: 4.367\n",
      "epoch: 0, train loss: 4.337\n",
      "epoch: 0, train loss: 4.343\n",
      "epoch: 0, train loss: 4.316\n",
      "epoch: 0, train loss: 4.285\n",
      "epoch: 0, train loss: 4.273\n",
      "epoch: 0, train loss: 4.274\n",
      "epoch: 0, train loss: 4.239\n",
      "epoch: 0, train loss: 4.198\n",
      "epoch: 0, train loss: 4.222\n",
      "epoch: 0, train loss: 4.250\n",
      "epoch: 0, train loss: 4.203\n",
      "epoch: 0, train loss: 4.159\n",
      "epoch: 0, train loss: 4.160\n",
      "epoch: 0, train loss: 4.131\n",
      "epoch: 0, train loss: 4.114\n",
      "epoch: 0, train loss: 4.111\n",
      "epoch: 0, train loss: 4.078\n",
      "epoch: 0, train loss: 4.079\n",
      "epoch: 0, train loss: 4.092\n",
      "epoch: 0, train loss: 4.070\n",
      "epoch: 0, train loss: 4.053\n",
      "epoch: 0, train loss: 4.050\n",
      "epoch: 0, train loss: 4.038\n",
      "epoch: 0, train loss: 4.016\n",
      "epoch: 0, train loss: 4.029\n",
      "epoch: 0, train loss: 3.973\n",
      "epoch: 0, train loss: 3.988\n",
      "epoch: 0, train loss: 3.998\n",
      "epoch: 0, train loss: 4.023\n",
      "epoch: 0, train loss: 3.996\n",
      "epoch: 0, train loss: 3.939\n",
      "epoch: 0, train loss: 3.958\n",
      "epoch: 0, train loss: 3.917\n",
      "epoch: 0, train loss: 3.923\n",
      "epoch: 0, train loss: 3.893\n",
      "epoch: 0, train loss: 3.904\n",
      "epoch: 0, train loss: 3.883\n",
      "epoch: 0, train loss: 3.857\n",
      "epoch: 0, train loss: 3.865\n",
      "epoch: 0, train loss: 3.848\n",
      "epoch: 0, train loss: 3.816\n",
      "epoch: 0, train loss: 3.852\n",
      "epoch: 0, train loss: 3.840\n",
      "epoch: 0, train loss: 3.806\n",
      "epoch: 0, train loss: 3.822\n",
      "epoch: 0, train loss: 3.794\n",
      "epoch: 0, train loss: 3.794\n",
      "epoch: 0, train loss: 3.840\n",
      "epoch: 0, train loss: 3.794\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de4cc9c4235470c9bf739517b93d0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 1500\n",
      "epoch: 0, train loss: 56.162, val loss: 3.660\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c158c92cfea848e7b01594022f9aa524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train loss: 0.249\n",
      "epoch: 1, train loss: 3.757\n",
      "epoch: 1, train loss: 3.722\n",
      "epoch: 1, train loss: 3.754\n",
      "epoch: 1, train loss: 3.743\n",
      "epoch: 1, train loss: 3.706\n",
      "epoch: 1, train loss: 3.720\n",
      "epoch: 1, train loss: 3.745\n",
      "epoch: 1, train loss: 3.716\n",
      "epoch: 1, train loss: 3.677\n",
      "epoch: 1, train loss: 3.713\n",
      "epoch: 1, train loss: 3.732\n",
      "epoch: 1, train loss: 3.682\n",
      "epoch: 1, train loss: 3.696\n",
      "epoch: 1, train loss: 3.675\n",
      "epoch: 1, train loss: 3.623\n",
      "epoch: 1, train loss: 3.674\n",
      "epoch: 1, train loss: 3.636\n",
      "epoch: 1, train loss: 3.652\n",
      "epoch: 1, train loss: 3.630\n",
      "epoch: 1, train loss: 3.617\n",
      "epoch: 1, train loss: 3.605\n",
      "epoch: 1, train loss: 3.596\n",
      "epoch: 1, train loss: 3.609\n",
      "epoch: 1, train loss: 3.605\n",
      "epoch: 1, train loss: 3.611\n",
      "epoch: 1, train loss: 3.604\n",
      "epoch: 1, train loss: 3.583\n",
      "epoch: 1, train loss: 3.569\n",
      "epoch: 1, train loss: 3.576\n",
      "epoch: 1, train loss: 3.547\n",
      "epoch: 1, train loss: 3.556\n",
      "epoch: 1, train loss: 3.536\n",
      "epoch: 1, train loss: 3.519\n",
      "epoch: 1, train loss: 3.489\n",
      "epoch: 1, train loss: 3.530\n",
      "epoch: 1, train loss: 3.493\n",
      "epoch: 1, train loss: 3.495\n",
      "epoch: 1, train loss: 3.512\n",
      "epoch: 1, train loss: 3.503\n",
      "epoch: 1, train loss: 3.489\n",
      "epoch: 1, train loss: 3.486\n",
      "epoch: 1, train loss: 3.443\n",
      "epoch: 1, train loss: 3.498\n",
      "epoch: 1, train loss: 3.447\n",
      "epoch: 1, train loss: 3.421\n",
      "epoch: 1, train loss: 3.413\n",
      "epoch: 1, train loss: 3.439\n",
      "epoch: 1, train loss: 3.422\n",
      "epoch: 1, train loss: 3.431\n",
      "epoch: 1, train loss: 3.412\n",
      "epoch: 1, train loss: 3.420\n",
      "epoch: 1, train loss: 3.406\n",
      "epoch: 1, train loss: 3.408\n",
      "epoch: 1, train loss: 3.372\n",
      "epoch: 1, train loss: 3.381\n",
      "epoch: 1, train loss: 3.394\n",
      "epoch: 1, train loss: 3.365\n",
      "epoch: 1, train loss: 3.374\n",
      "epoch: 1, train loss: 3.370\n",
      "epoch: 1, train loss: 3.357\n",
      "epoch: 1, train loss: 3.345\n",
      "epoch: 1, train loss: 3.300\n",
      "epoch: 1, train loss: 3.323\n",
      "epoch: 1, train loss: 3.355\n",
      "epoch: 1, train loss: 3.339\n",
      "epoch: 1, train loss: 3.281\n",
      "epoch: 1, train loss: 3.370\n",
      "epoch: 1, train loss: 3.316\n",
      "epoch: 1, train loss: 3.323\n",
      "epoch: 1, train loss: 3.287\n",
      "epoch: 1, train loss: 3.278\n",
      "epoch: 1, train loss: 3.292\n",
      "epoch: 1, train loss: 3.277\n",
      "epoch: 1, train loss: 3.266\n",
      "epoch: 1, train loss: 3.272\n",
      "epoch: 1, train loss: 3.255\n",
      "epoch: 1, train loss: 3.246\n",
      "epoch: 1, train loss: 3.236\n",
      "epoch: 1, train loss: 3.247\n",
      "epoch: 1, train loss: 3.272\n",
      "epoch: 1, train loss: 3.228\n",
      "epoch: 1, train loss: 3.228\n",
      "epoch: 1, train loss: 3.227\n",
      "epoch: 1, train loss: 3.229\n",
      "epoch: 1, train loss: 3.199\n",
      "epoch: 1, train loss: 3.216\n",
      "epoch: 1, train loss: 3.203\n",
      "epoch: 1, train loss: 3.199\n",
      "epoch: 1, train loss: 3.211\n",
      "epoch: 1, train loss: 3.200\n",
      "epoch: 1, train loss: 3.165\n",
      "epoch: 1, train loss: 3.107\n",
      "epoch: 1, train loss: 3.148\n",
      "epoch: 1, train loss: 3.167\n",
      "epoch: 1, train loss: 3.142\n",
      "epoch: 1, train loss: 3.173\n",
      "epoch: 1, train loss: 3.097\n",
      "epoch: 1, train loss: 3.164\n",
      "epoch: 1, train loss: 3.119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932ac6cc7ddd471ea566d871c0812e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 3000\n",
      "epoch: 1, train loss: 46.602, val loss: 3.004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303757d71028437da4024fa3d5c77a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, train loss: 0.197\n",
      "epoch: 2, train loss: 3.100\n",
      "epoch: 2, train loss: 3.120\n",
      "epoch: 2, train loss: 3.130\n",
      "epoch: 2, train loss: 3.092\n",
      "epoch: 2, train loss: 3.095\n",
      "epoch: 2, train loss: 3.059\n",
      "epoch: 2, train loss: 3.084\n",
      "epoch: 2, train loss: 3.088\n",
      "epoch: 2, train loss: 3.101\n",
      "epoch: 2, train loss: 3.101\n",
      "epoch: 2, train loss: 3.078\n",
      "epoch: 2, train loss: 3.069\n",
      "epoch: 2, train loss: 3.063\n",
      "epoch: 2, train loss: 3.061\n",
      "epoch: 2, train loss: 3.058\n",
      "epoch: 2, train loss: 3.073\n",
      "epoch: 2, train loss: 3.044\n",
      "epoch: 2, train loss: 3.059\n",
      "epoch: 2, train loss: 3.029\n",
      "epoch: 2, train loss: 3.043\n",
      "epoch: 2, train loss: 3.029\n",
      "epoch: 2, train loss: 3.074\n",
      "epoch: 2, train loss: 3.023\n",
      "epoch: 2, train loss: 3.005\n",
      "epoch: 2, train loss: 3.027\n",
      "epoch: 2, train loss: 3.024\n",
      "epoch: 2, train loss: 3.010\n",
      "epoch: 2, train loss: 2.995\n",
      "epoch: 2, train loss: 2.984\n",
      "epoch: 2, train loss: 3.013\n",
      "epoch: 2, train loss: 3.001\n",
      "epoch: 2, train loss: 2.960\n",
      "epoch: 2, train loss: 2.952\n",
      "epoch: 2, train loss: 3.023\n",
      "epoch: 2, train loss: 3.018\n",
      "epoch: 2, train loss: 2.972\n",
      "epoch: 2, train loss: 2.959\n",
      "epoch: 2, train loss: 2.957\n",
      "epoch: 2, train loss: 2.923\n",
      "epoch: 2, train loss: 2.950\n",
      "epoch: 2, train loss: 2.957\n",
      "epoch: 2, train loss: 2.930\n",
      "epoch: 2, train loss: 2.916\n",
      "epoch: 2, train loss: 2.907\n",
      "epoch: 2, train loss: 2.929\n",
      "epoch: 2, train loss: 2.926\n",
      "epoch: 2, train loss: 2.923\n",
      "epoch: 2, train loss: 2.900\n",
      "epoch: 2, train loss: 2.912\n",
      "epoch: 2, train loss: 2.924\n",
      "epoch: 2, train loss: 2.917\n",
      "epoch: 2, train loss: 2.887\n",
      "epoch: 2, train loss: 2.920\n",
      "epoch: 2, train loss: 2.911\n",
      "epoch: 2, train loss: 2.891\n",
      "epoch: 2, train loss: 2.895\n",
      "epoch: 2, train loss: 2.862\n",
      "epoch: 2, train loss: 2.873\n",
      "epoch: 2, train loss: 2.878\n",
      "epoch: 2, train loss: 2.864\n",
      "epoch: 2, train loss: 2.854\n",
      "epoch: 2, train loss: 2.855\n",
      "epoch: 2, train loss: 2.861\n",
      "epoch: 2, train loss: 2.876\n",
      "epoch: 2, train loss: 2.810\n",
      "epoch: 2, train loss: 2.878\n",
      "epoch: 2, train loss: 2.830\n",
      "epoch: 2, train loss: 2.825\n",
      "epoch: 2, train loss: 2.825\n",
      "epoch: 2, train loss: 2.846\n",
      "epoch: 2, train loss: 2.808\n",
      "epoch: 2, train loss: 2.816\n",
      "epoch: 2, train loss: 2.802\n",
      "epoch: 2, train loss: 2.807\n",
      "epoch: 2, train loss: 2.802\n",
      "epoch: 2, train loss: 2.795\n",
      "epoch: 2, train loss: 2.807\n",
      "epoch: 2, train loss: 2.806\n",
      "epoch: 2, train loss: 2.769\n",
      "epoch: 2, train loss: 2.792\n",
      "epoch: 2, train loss: 2.765\n",
      "epoch: 2, train loss: 2.791\n",
      "epoch: 2, train loss: 2.792\n",
      "epoch: 2, train loss: 2.752\n",
      "epoch: 2, train loss: 2.803\n",
      "epoch: 2, train loss: 2.777\n",
      "epoch: 2, train loss: 2.784\n",
      "epoch: 2, train loss: 2.766\n",
      "epoch: 2, train loss: 2.742\n",
      "epoch: 2, train loss: 2.757\n",
      "epoch: 2, train loss: 2.755\n",
      "epoch: 2, train loss: 2.767\n",
      "epoch: 2, train loss: 2.737\n",
      "epoch: 2, train loss: 2.743\n",
      "epoch: 2, train loss: 2.758\n",
      "epoch: 2, train loss: 2.748\n",
      "epoch: 2, train loss: 2.712\n",
      "epoch: 2, train loss: 2.741\n",
      "epoch: 2, train loss: 2.726\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9a5fa49fa74a60a7e3c47237b10bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 4500\n",
      "epoch: 2, train loss: 40.484, val loss: 2.596\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0891263bf6734ff1bbdfdf93c3cf714e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, train loss: 0.172\n",
      "epoch: 3, train loss: 2.682\n",
      "epoch: 3, train loss: 2.762\n",
      "epoch: 3, train loss: 2.714\n",
      "epoch: 3, train loss: 2.697\n",
      "epoch: 3, train loss: 2.743\n",
      "epoch: 3, train loss: 2.718\n",
      "epoch: 3, train loss: 2.673\n",
      "epoch: 3, train loss: 2.704\n",
      "epoch: 3, train loss: 2.693\n",
      "epoch: 3, train loss: 2.683\n",
      "epoch: 3, train loss: 2.675\n",
      "epoch: 3, train loss: 2.670\n",
      "epoch: 3, train loss: 2.667\n",
      "epoch: 3, train loss: 2.701\n",
      "epoch: 3, train loss: 2.649\n",
      "epoch: 3, train loss: 2.654\n",
      "epoch: 3, train loss: 2.699\n",
      "epoch: 3, train loss: 2.667\n",
      "epoch: 3, train loss: 2.652\n",
      "epoch: 3, train loss: 2.655\n",
      "epoch: 3, train loss: 2.652\n",
      "epoch: 3, train loss: 2.679\n",
      "epoch: 3, train loss: 2.619\n",
      "epoch: 3, train loss: 2.623\n",
      "epoch: 3, train loss: 2.651\n",
      "epoch: 3, train loss: 2.635\n",
      "epoch: 3, train loss: 2.641\n",
      "epoch: 3, train loss: 2.622\n",
      "epoch: 3, train loss: 2.626\n",
      "epoch: 3, train loss: 2.636\n",
      "epoch: 3, train loss: 2.599\n",
      "epoch: 3, train loss: 2.607\n",
      "epoch: 3, train loss: 2.648\n",
      "epoch: 3, train loss: 2.615\n",
      "epoch: 3, train loss: 2.591\n",
      "epoch: 3, train loss: 2.583\n",
      "epoch: 3, train loss: 2.576\n",
      "epoch: 3, train loss: 2.615\n",
      "epoch: 3, train loss: 2.599\n",
      "epoch: 3, train loss: 2.599\n",
      "epoch: 3, train loss: 2.603\n",
      "epoch: 3, train loss: 2.568\n",
      "epoch: 3, train loss: 2.566\n",
      "epoch: 3, train loss: 2.599\n",
      "epoch: 3, train loss: 2.578\n",
      "epoch: 3, train loss: 2.585\n",
      "epoch: 3, train loss: 2.564\n",
      "epoch: 3, train loss: 2.567\n",
      "epoch: 3, train loss: 2.544\n",
      "epoch: 3, train loss: 2.559\n",
      "epoch: 3, train loss: 2.560\n",
      "epoch: 3, train loss: 2.544\n",
      "epoch: 3, train loss: 2.529\n",
      "epoch: 3, train loss: 2.563\n",
      "epoch: 3, train loss: 2.549\n",
      "epoch: 3, train loss: 2.510\n",
      "epoch: 3, train loss: 2.551\n",
      "epoch: 3, train loss: 2.535\n",
      "epoch: 3, train loss: 2.515\n",
      "epoch: 3, train loss: 2.518\n",
      "epoch: 3, train loss: 2.555\n",
      "epoch: 3, train loss: 2.523\n",
      "epoch: 3, train loss: 2.488\n",
      "epoch: 3, train loss: 2.540\n",
      "epoch: 3, train loss: 2.514\n",
      "epoch: 3, train loss: 2.510\n",
      "epoch: 3, train loss: 2.499\n",
      "epoch: 3, train loss: 2.524\n",
      "epoch: 3, train loss: 2.502\n",
      "epoch: 3, train loss: 2.500\n",
      "epoch: 3, train loss: 2.525\n",
      "epoch: 3, train loss: 2.497\n",
      "epoch: 3, train loss: 2.465\n",
      "epoch: 3, train loss: 2.492\n",
      "epoch: 3, train loss: 2.477\n",
      "epoch: 3, train loss: 2.499\n",
      "epoch: 3, train loss: 2.474\n",
      "epoch: 3, train loss: 2.510\n",
      "epoch: 3, train loss: 2.458\n",
      "epoch: 3, train loss: 2.478\n",
      "epoch: 3, train loss: 2.486\n",
      "epoch: 3, train loss: 2.441\n",
      "epoch: 3, train loss: 2.482\n",
      "epoch: 3, train loss: 2.445\n",
      "epoch: 3, train loss: 2.494\n",
      "epoch: 3, train loss: 2.439\n",
      "epoch: 3, train loss: 2.484\n",
      "epoch: 3, train loss: 2.481\n",
      "epoch: 3, train loss: 2.426\n",
      "epoch: 3, train loss: 2.460\n",
      "epoch: 3, train loss: 2.447\n",
      "epoch: 3, train loss: 2.449\n",
      "epoch: 3, train loss: 2.447\n",
      "epoch: 3, train loss: 2.438\n",
      "epoch: 3, train loss: 2.452\n",
      "epoch: 3, train loss: 2.437\n",
      "epoch: 3, train loss: 2.435\n",
      "epoch: 3, train loss: 2.402\n",
      "epoch: 3, train loss: 2.380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e32d038f1d4422b2337ff19a818cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 6000\n",
      "epoch: 3, train loss: 36.501, val loss: 2.294\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6e8060ad144ffcacdd9a5018b84004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, train loss: 0.159\n",
      "epoch: 4, train loss: 2.434\n",
      "epoch: 4, train loss: 2.433\n",
      "epoch: 4, train loss: 2.386\n",
      "epoch: 4, train loss: 2.401\n",
      "epoch: 4, train loss: 2.401\n",
      "epoch: 4, train loss: 2.411\n",
      "epoch: 4, train loss: 2.418\n",
      "epoch: 4, train loss: 2.380\n",
      "epoch: 4, train loss: 2.389\n",
      "epoch: 4, train loss: 2.372\n",
      "epoch: 4, train loss: 2.362\n",
      "epoch: 4, train loss: 2.407\n",
      "epoch: 4, train loss: 2.380\n",
      "epoch: 4, train loss: 2.392\n",
      "epoch: 4, train loss: 2.431\n",
      "epoch: 4, train loss: 2.374\n",
      "epoch: 4, train loss: 2.349\n",
      "epoch: 4, train loss: 2.367\n",
      "epoch: 4, train loss: 2.370\n",
      "epoch: 4, train loss: 2.340\n",
      "epoch: 4, train loss: 2.344\n",
      "epoch: 4, train loss: 2.389\n",
      "epoch: 4, train loss: 2.368\n",
      "epoch: 4, train loss: 2.377\n",
      "epoch: 4, train loss: 2.350\n",
      "epoch: 4, train loss: 2.338\n",
      "epoch: 4, train loss: 2.322\n",
      "epoch: 4, train loss: 2.352\n",
      "epoch: 4, train loss: 2.346\n",
      "epoch: 4, train loss: 2.318\n",
      "epoch: 4, train loss: 2.358\n",
      "epoch: 4, train loss: 2.353\n",
      "epoch: 4, train loss: 2.343\n",
      "epoch: 4, train loss: 2.312\n",
      "epoch: 4, train loss: 2.332\n",
      "epoch: 4, train loss: 2.318\n",
      "epoch: 4, train loss: 2.338\n",
      "epoch: 4, train loss: 2.318\n",
      "epoch: 4, train loss: 2.360\n",
      "epoch: 4, train loss: 2.324\n",
      "epoch: 4, train loss: 2.284\n",
      "epoch: 4, train loss: 2.307\n",
      "epoch: 4, train loss: 2.319\n",
      "epoch: 4, train loss: 2.300\n",
      "epoch: 4, train loss: 2.309\n",
      "epoch: 4, train loss: 2.289\n",
      "epoch: 4, train loss: 2.281\n",
      "epoch: 4, train loss: 2.320\n",
      "epoch: 4, train loss: 2.307\n",
      "epoch: 4, train loss: 2.313\n",
      "epoch: 4, train loss: 2.266\n",
      "epoch: 4, train loss: 2.286\n",
      "epoch: 4, train loss: 2.260\n",
      "epoch: 4, train loss: 2.275\n",
      "epoch: 4, train loss: 2.255\n",
      "epoch: 4, train loss: 2.285\n",
      "epoch: 4, train loss: 2.293\n",
      "epoch: 4, train loss: 2.256\n",
      "epoch: 4, train loss: 2.262\n",
      "epoch: 4, train loss: 2.282\n",
      "epoch: 4, train loss: 2.251\n",
      "epoch: 4, train loss: 2.286\n",
      "epoch: 4, train loss: 2.257\n",
      "epoch: 4, train loss: 2.263\n",
      "epoch: 4, train loss: 2.245\n",
      "epoch: 4, train loss: 2.234\n",
      "epoch: 4, train loss: 2.258\n",
      "epoch: 4, train loss: 2.233\n",
      "epoch: 4, train loss: 2.261\n",
      "epoch: 4, train loss: 2.224\n",
      "epoch: 4, train loss: 2.253\n",
      "epoch: 4, train loss: 2.236\n",
      "epoch: 4, train loss: 2.225\n",
      "epoch: 4, train loss: 2.269\n",
      "epoch: 4, train loss: 2.247\n",
      "epoch: 4, train loss: 2.222\n",
      "epoch: 4, train loss: 2.259\n",
      "epoch: 4, train loss: 2.221\n",
      "epoch: 4, train loss: 2.230\n",
      "epoch: 4, train loss: 2.227\n",
      "epoch: 4, train loss: 2.226\n",
      "epoch: 4, train loss: 2.225\n",
      "epoch: 4, train loss: 2.218\n",
      "epoch: 4, train loss: 2.197\n",
      "epoch: 4, train loss: 2.178\n",
      "epoch: 4, train loss: 2.223\n",
      "epoch: 4, train loss: 2.230\n",
      "epoch: 4, train loss: 2.189\n",
      "epoch: 4, train loss: 2.176\n",
      "epoch: 4, train loss: 2.231\n",
      "epoch: 4, train loss: 2.205\n",
      "epoch: 4, train loss: 2.245\n",
      "epoch: 4, train loss: 2.149\n",
      "epoch: 4, train loss: 2.206\n",
      "epoch: 4, train loss: 2.174\n",
      "epoch: 4, train loss: 2.148\n",
      "epoch: 4, train loss: 2.196\n",
      "epoch: 4, train loss: 2.196\n",
      "epoch: 4, train loss: 2.201\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a380da840c834a48923f47b3861a6e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 7500\n",
      "epoch: 4, train loss: 32.740, val loss: 2.058\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554e468b70f0494ca0c22b9372a4eef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, train loss: 0.141\n",
      "epoch: 5, train loss: 2.171\n",
      "epoch: 5, train loss: 2.172\n",
      "epoch: 5, train loss: 2.164\n",
      "epoch: 5, train loss: 2.159\n",
      "epoch: 5, train loss: 2.163\n",
      "epoch: 5, train loss: 2.159\n",
      "epoch: 5, train loss: 2.154\n",
      "epoch: 5, train loss: 2.167\n",
      "epoch: 5, train loss: 2.179\n",
      "epoch: 5, train loss: 2.180\n",
      "epoch: 5, train loss: 2.160\n",
      "epoch: 5, train loss: 2.158\n",
      "epoch: 5, train loss: 2.148\n",
      "epoch: 5, train loss: 2.174\n",
      "epoch: 5, train loss: 2.137\n",
      "epoch: 5, train loss: 2.157\n",
      "epoch: 5, train loss: 2.171\n",
      "epoch: 5, train loss: 2.159\n",
      "epoch: 5, train loss: 2.134\n",
      "epoch: 5, train loss: 2.179\n",
      "epoch: 5, train loss: 2.143\n",
      "epoch: 5, train loss: 2.100\n",
      "epoch: 5, train loss: 2.109\n",
      "epoch: 5, train loss: 2.161\n",
      "epoch: 5, train loss: 2.132\n",
      "epoch: 5, train loss: 2.131\n",
      "epoch: 5, train loss: 2.142\n",
      "epoch: 5, train loss: 2.115\n",
      "epoch: 5, train loss: 2.105\n",
      "epoch: 5, train loss: 2.117\n",
      "epoch: 5, train loss: 2.145\n",
      "epoch: 5, train loss: 2.169\n",
      "epoch: 5, train loss: 2.136\n",
      "epoch: 5, train loss: 2.112\n",
      "epoch: 5, train loss: 2.125\n",
      "epoch: 5, train loss: 2.126\n",
      "epoch: 5, train loss: 2.144\n",
      "epoch: 5, train loss: 2.108\n",
      "epoch: 5, train loss: 2.108\n",
      "epoch: 5, train loss: 2.092\n",
      "epoch: 5, train loss: 2.073\n",
      "epoch: 5, train loss: 2.079\n",
      "epoch: 5, train loss: 2.129\n",
      "epoch: 5, train loss: 2.113\n",
      "epoch: 5, train loss: 2.128\n",
      "epoch: 5, train loss: 2.095\n",
      "epoch: 5, train loss: 2.108\n",
      "epoch: 5, train loss: 2.086\n",
      "epoch: 5, train loss: 2.070\n",
      "epoch: 5, train loss: 2.127\n",
      "epoch: 5, train loss: 2.113\n",
      "epoch: 5, train loss: 2.065\n",
      "epoch: 5, train loss: 2.083\n",
      "epoch: 5, train loss: 2.085\n",
      "epoch: 5, train loss: 2.119\n",
      "epoch: 5, train loss: 2.107\n",
      "epoch: 5, train loss: 2.104\n",
      "epoch: 5, train loss: 2.107\n",
      "epoch: 5, train loss: 2.093\n",
      "epoch: 5, train loss: 2.102\n",
      "epoch: 5, train loss: 2.075\n",
      "epoch: 5, train loss: 2.093\n",
      "epoch: 5, train loss: 2.071\n",
      "epoch: 5, train loss: 2.062\n",
      "epoch: 5, train loss: 2.071\n",
      "epoch: 5, train loss: 2.079\n",
      "epoch: 5, train loss: 2.040\n",
      "epoch: 5, train loss: 2.060\n",
      "epoch: 5, train loss: 2.071\n",
      "epoch: 5, train loss: 2.051\n",
      "epoch: 5, train loss: 2.054\n",
      "epoch: 5, train loss: 2.045\n",
      "epoch: 5, train loss: 2.078\n",
      "epoch: 5, train loss: 2.058\n",
      "epoch: 5, train loss: 2.066\n",
      "epoch: 5, train loss: 2.076\n",
      "epoch: 5, train loss: 2.054\n",
      "epoch: 5, train loss: 2.050\n",
      "epoch: 5, train loss: 2.059\n",
      "epoch: 5, train loss: 2.054\n",
      "epoch: 5, train loss: 2.036\n",
      "epoch: 5, train loss: 2.050\n",
      "epoch: 5, train loss: 2.072\n",
      "epoch: 5, train loss: 2.085\n",
      "epoch: 5, train loss: 2.020\n",
      "epoch: 5, train loss: 2.037\n",
      "epoch: 5, train loss: 2.065\n",
      "epoch: 5, train loss: 2.046\n",
      "epoch: 5, train loss: 2.022\n",
      "epoch: 5, train loss: 2.050\n",
      "epoch: 5, train loss: 2.026\n",
      "epoch: 5, train loss: 2.043\n",
      "epoch: 5, train loss: 2.020\n",
      "epoch: 5, train loss: 2.023\n",
      "epoch: 5, train loss: 2.028\n",
      "epoch: 5, train loss: 2.040\n",
      "epoch: 5, train loss: 2.036\n",
      "epoch: 5, train loss: 2.037\n",
      "epoch: 5, train loss: 2.016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0a7017701047b595b4a942b255d3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 9000\n",
      "epoch: 5, train loss: 30.334, val loss: 1.911\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3953ecf691b412ca62eade89eded15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, train loss: 0.129\n",
      "epoch: 6, train loss: 1.997\n",
      "epoch: 6, train loss: 2.019\n",
      "epoch: 6, train loss: 2.004\n",
      "epoch: 6, train loss: 2.004\n",
      "epoch: 6, train loss: 1.992\n",
      "epoch: 6, train loss: 2.041\n",
      "epoch: 6, train loss: 2.040\n",
      "epoch: 6, train loss: 2.019\n",
      "epoch: 6, train loss: 2.026\n",
      "epoch: 6, train loss: 2.003\n",
      "epoch: 6, train loss: 2.020\n",
      "epoch: 6, train loss: 1.986\n",
      "epoch: 6, train loss: 1.986\n",
      "epoch: 6, train loss: 2.004\n",
      "epoch: 6, train loss: 1.970\n",
      "epoch: 6, train loss: 1.999\n",
      "epoch: 6, train loss: 1.992\n",
      "epoch: 6, train loss: 1.992\n",
      "epoch: 6, train loss: 1.997\n",
      "epoch: 6, train loss: 2.007\n",
      "epoch: 6, train loss: 1.983\n",
      "epoch: 6, train loss: 1.996\n",
      "epoch: 6, train loss: 1.999\n",
      "epoch: 6, train loss: 1.997\n",
      "epoch: 6, train loss: 1.980\n",
      "epoch: 6, train loss: 1.980\n",
      "epoch: 6, train loss: 1.984\n",
      "epoch: 6, train loss: 1.982\n",
      "epoch: 6, train loss: 1.986\n",
      "epoch: 6, train loss: 2.015\n",
      "epoch: 6, train loss: 1.988\n",
      "epoch: 6, train loss: 1.991\n",
      "epoch: 6, train loss: 1.998\n",
      "epoch: 6, train loss: 1.973\n",
      "epoch: 6, train loss: 1.994\n",
      "epoch: 6, train loss: 1.970\n",
      "epoch: 6, train loss: 1.943\n",
      "epoch: 6, train loss: 1.986\n",
      "epoch: 6, train loss: 2.005\n",
      "epoch: 6, train loss: 1.963\n",
      "epoch: 6, train loss: 1.989\n",
      "epoch: 6, train loss: 1.972\n",
      "epoch: 6, train loss: 1.978\n",
      "epoch: 6, train loss: 1.977\n",
      "epoch: 6, train loss: 2.005\n",
      "epoch: 6, train loss: 1.960\n",
      "epoch: 6, train loss: 1.979\n",
      "epoch: 6, train loss: 1.971\n",
      "epoch: 6, train loss: 1.969\n",
      "epoch: 6, train loss: 1.982\n",
      "epoch: 6, train loss: 1.967\n",
      "epoch: 6, train loss: 1.938\n",
      "epoch: 6, train loss: 1.982\n",
      "epoch: 6, train loss: 1.975\n",
      "epoch: 6, train loss: 1.947\n",
      "epoch: 6, train loss: 1.972\n",
      "epoch: 6, train loss: 1.959\n",
      "epoch: 6, train loss: 1.954\n",
      "epoch: 6, train loss: 1.949\n",
      "epoch: 6, train loss: 1.947\n",
      "epoch: 6, train loss: 1.952\n",
      "epoch: 6, train loss: 1.943\n",
      "epoch: 6, train loss: 1.965\n",
      "epoch: 6, train loss: 1.956\n",
      "epoch: 6, train loss: 1.945\n",
      "epoch: 6, train loss: 1.908\n",
      "epoch: 6, train loss: 1.956\n",
      "epoch: 6, train loss: 1.931\n",
      "epoch: 6, train loss: 1.956\n",
      "epoch: 6, train loss: 1.929\n",
      "epoch: 6, train loss: 1.967\n",
      "epoch: 6, train loss: 1.925\n",
      "epoch: 6, train loss: 1.959\n",
      "epoch: 6, train loss: 1.930\n",
      "epoch: 6, train loss: 1.968\n",
      "epoch: 6, train loss: 1.925\n",
      "epoch: 6, train loss: 1.938\n",
      "epoch: 6, train loss: 1.935\n",
      "epoch: 6, train loss: 1.928\n",
      "epoch: 6, train loss: 1.913\n",
      "epoch: 6, train loss: 1.932\n",
      "epoch: 6, train loss: 1.893\n",
      "epoch: 6, train loss: 1.930\n",
      "epoch: 6, train loss: 1.917\n",
      "epoch: 6, train loss: 1.958\n",
      "epoch: 6, train loss: 1.924\n",
      "epoch: 6, train loss: 1.947\n",
      "epoch: 6, train loss: 1.936\n",
      "epoch: 6, train loss: 1.941\n",
      "epoch: 6, train loss: 1.904\n",
      "epoch: 6, train loss: 1.929\n",
      "epoch: 6, train loss: 1.917\n",
      "epoch: 6, train loss: 1.924\n",
      "epoch: 6, train loss: 1.901\n",
      "epoch: 6, train loss: 1.902\n",
      "epoch: 6, train loss: 1.922\n",
      "epoch: 6, train loss: 1.910\n",
      "epoch: 6, train loss: 1.925\n",
      "epoch: 6, train loss: 1.887\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc683987f6514fa389f93c5a4136f26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 10500\n",
      "epoch: 6, train loss: 28.705, val loss: 1.806\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708d3d032e554e54af95a10f3f82c5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, train loss: 0.125\n",
      "epoch: 7, train loss: 1.910\n",
      "epoch: 7, train loss: 1.912\n",
      "epoch: 7, train loss: 1.930\n",
      "epoch: 7, train loss: 1.930\n",
      "epoch: 7, train loss: 1.896\n",
      "epoch: 7, train loss: 1.901\n",
      "epoch: 7, train loss: 1.896\n",
      "epoch: 7, train loss: 1.928\n",
      "epoch: 7, train loss: 1.918\n",
      "epoch: 7, train loss: 1.903\n",
      "epoch: 7, train loss: 1.909\n",
      "epoch: 7, train loss: 1.918\n",
      "epoch: 7, train loss: 1.910\n",
      "epoch: 7, train loss: 1.907\n",
      "epoch: 7, train loss: 1.909\n",
      "epoch: 7, train loss: 1.922\n",
      "epoch: 7, train loss: 1.903\n",
      "epoch: 7, train loss: 1.912\n",
      "epoch: 7, train loss: 1.851\n",
      "epoch: 7, train loss: 1.890\n",
      "epoch: 7, train loss: 1.899\n",
      "epoch: 7, train loss: 1.874\n",
      "epoch: 7, train loss: 1.885\n",
      "epoch: 7, train loss: 1.875\n",
      "epoch: 7, train loss: 1.887\n",
      "epoch: 7, train loss: 1.891\n",
      "epoch: 7, train loss: 1.916\n",
      "epoch: 7, train loss: 1.868\n",
      "epoch: 7, train loss: 1.906\n",
      "epoch: 7, train loss: 1.912\n",
      "epoch: 7, train loss: 1.889\n",
      "epoch: 7, train loss: 1.865\n",
      "epoch: 7, train loss: 1.871\n",
      "epoch: 7, train loss: 1.898\n",
      "epoch: 7, train loss: 1.923\n",
      "epoch: 7, train loss: 1.894\n",
      "epoch: 7, train loss: 1.905\n",
      "epoch: 7, train loss: 1.823\n",
      "epoch: 7, train loss: 1.859\n",
      "epoch: 7, train loss: 1.898\n",
      "epoch: 7, train loss: 1.876\n",
      "epoch: 7, train loss: 1.875\n",
      "epoch: 7, train loss: 1.867\n",
      "epoch: 7, train loss: 1.843\n",
      "epoch: 7, train loss: 1.860\n",
      "epoch: 7, train loss: 1.865\n",
      "epoch: 7, train loss: 1.889\n",
      "epoch: 7, train loss: 1.877\n",
      "epoch: 7, train loss: 1.890\n",
      "epoch: 7, train loss: 1.859\n",
      "epoch: 7, train loss: 1.840\n",
      "epoch: 7, train loss: 1.838\n",
      "epoch: 7, train loss: 1.867\n",
      "epoch: 7, train loss: 1.881\n",
      "epoch: 7, train loss: 1.868\n",
      "epoch: 7, train loss: 1.865\n",
      "epoch: 7, train loss: 1.865\n",
      "epoch: 7, train loss: 1.842\n",
      "epoch: 7, train loss: 1.860\n",
      "epoch: 7, train loss: 1.823\n",
      "epoch: 7, train loss: 1.850\n",
      "epoch: 7, train loss: 1.867\n",
      "epoch: 7, train loss: 1.843\n",
      "epoch: 7, train loss: 1.852\n",
      "epoch: 7, train loss: 1.880\n",
      "epoch: 7, train loss: 1.821\n",
      "epoch: 7, train loss: 1.853\n",
      "epoch: 7, train loss: 1.882\n",
      "epoch: 7, train loss: 1.806\n",
      "epoch: 7, train loss: 1.867\n",
      "epoch: 7, train loss: 1.865\n",
      "epoch: 7, train loss: 1.798\n",
      "epoch: 7, train loss: 1.869\n",
      "epoch: 7, train loss: 1.828\n",
      "epoch: 7, train loss: 1.870\n",
      "epoch: 7, train loss: 1.830\n",
      "epoch: 7, train loss: 1.853\n",
      "epoch: 7, train loss: 1.829\n",
      "epoch: 7, train loss: 1.838\n",
      "epoch: 7, train loss: 1.833\n",
      "epoch: 7, train loss: 1.857\n",
      "epoch: 7, train loss: 1.845\n",
      "epoch: 7, train loss: 1.833\n",
      "epoch: 7, train loss: 1.807\n",
      "epoch: 7, train loss: 1.857\n",
      "epoch: 7, train loss: 1.860\n",
      "epoch: 7, train loss: 1.857\n",
      "epoch: 7, train loss: 1.827\n",
      "epoch: 7, train loss: 1.845\n",
      "epoch: 7, train loss: 1.831\n",
      "epoch: 7, train loss: 1.826\n",
      "epoch: 7, train loss: 1.825\n",
      "epoch: 7, train loss: 1.842\n",
      "epoch: 7, train loss: 1.822\n",
      "epoch: 7, train loss: 1.835\n",
      "epoch: 7, train loss: 1.839\n",
      "epoch: 7, train loss: 1.841\n",
      "epoch: 7, train loss: 1.817\n",
      "epoch: 7, train loss: 1.830\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236700a6b1114ae3bd46eb81fcf20a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 12000\n",
      "epoch: 7, train loss: 27.507, val loss: 1.729\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbf17025d3646869e4fbd24a8bf9aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, train loss: 0.125\n",
      "epoch: 8, train loss: 1.833\n",
      "epoch: 8, train loss: 1.838\n",
      "epoch: 8, train loss: 1.826\n",
      "epoch: 8, train loss: 1.835\n",
      "epoch: 8, train loss: 1.818\n",
      "epoch: 8, train loss: 1.809\n",
      "epoch: 8, train loss: 1.839\n",
      "epoch: 8, train loss: 1.859\n",
      "epoch: 8, train loss: 1.823\n",
      "epoch: 8, train loss: 1.813\n",
      "epoch: 8, train loss: 1.796\n",
      "epoch: 8, train loss: 1.811\n",
      "epoch: 8, train loss: 1.823\n",
      "epoch: 8, train loss: 1.829\n",
      "epoch: 8, train loss: 1.789\n",
      "epoch: 8, train loss: 1.833\n",
      "epoch: 8, train loss: 1.843\n",
      "epoch: 8, train loss: 1.822\n",
      "epoch: 8, train loss: 1.822\n",
      "epoch: 8, train loss: 1.819\n",
      "epoch: 8, train loss: 1.824\n",
      "epoch: 8, train loss: 1.792\n",
      "epoch: 8, train loss: 1.821\n",
      "epoch: 8, train loss: 1.822\n",
      "epoch: 8, train loss: 1.841\n",
      "epoch: 8, train loss: 1.840\n",
      "epoch: 8, train loss: 1.814\n",
      "epoch: 8, train loss: 1.824\n",
      "epoch: 8, train loss: 1.801\n",
      "epoch: 8, train loss: 1.830\n",
      "epoch: 8, train loss: 1.803\n",
      "epoch: 8, train loss: 1.811\n",
      "epoch: 8, train loss: 1.814\n",
      "epoch: 8, train loss: 1.778\n",
      "epoch: 8, train loss: 1.830\n",
      "epoch: 8, train loss: 1.802\n",
      "epoch: 8, train loss: 1.822\n",
      "epoch: 8, train loss: 1.811\n",
      "epoch: 8, train loss: 1.802\n",
      "epoch: 8, train loss: 1.784\n",
      "epoch: 8, train loss: 1.803\n",
      "epoch: 8, train loss: 1.797\n",
      "epoch: 8, train loss: 1.808\n",
      "epoch: 8, train loss: 1.815\n",
      "epoch: 8, train loss: 1.816\n",
      "epoch: 8, train loss: 1.795\n",
      "epoch: 8, train loss: 1.793\n",
      "epoch: 8, train loss: 1.782\n",
      "epoch: 8, train loss: 1.783\n",
      "epoch: 8, train loss: 1.779\n",
      "epoch: 8, train loss: 1.806\n",
      "epoch: 8, train loss: 1.844\n",
      "epoch: 8, train loss: 1.791\n",
      "epoch: 8, train loss: 1.771\n",
      "epoch: 8, train loss: 1.819\n",
      "epoch: 8, train loss: 1.787\n",
      "epoch: 8, train loss: 1.790\n",
      "epoch: 8, train loss: 1.767\n",
      "epoch: 8, train loss: 1.774\n",
      "epoch: 8, train loss: 1.770\n",
      "epoch: 8, train loss: 1.807\n",
      "epoch: 8, train loss: 1.782\n",
      "epoch: 8, train loss: 1.781\n",
      "epoch: 8, train loss: 1.776\n",
      "epoch: 8, train loss: 1.758\n",
      "epoch: 8, train loss: 1.773\n",
      "epoch: 8, train loss: 1.792\n",
      "epoch: 8, train loss: 1.790\n",
      "epoch: 8, train loss: 1.790\n",
      "epoch: 8, train loss: 1.771\n",
      "epoch: 8, train loss: 1.799\n",
      "epoch: 8, train loss: 1.752\n",
      "epoch: 8, train loss: 1.786\n",
      "epoch: 8, train loss: 1.786\n",
      "epoch: 8, train loss: 1.771\n",
      "epoch: 8, train loss: 1.762\n",
      "epoch: 8, train loss: 1.779\n",
      "epoch: 8, train loss: 1.763\n",
      "epoch: 8, train loss: 1.779\n",
      "epoch: 8, train loss: 1.769\n",
      "epoch: 8, train loss: 1.783\n",
      "epoch: 8, train loss: 1.777\n",
      "epoch: 8, train loss: 1.795\n",
      "epoch: 8, train loss: 1.788\n",
      "epoch: 8, train loss: 1.806\n",
      "epoch: 8, train loss: 1.770\n",
      "epoch: 8, train loss: 1.796\n",
      "epoch: 8, train loss: 1.775\n",
      "epoch: 8, train loss: 1.779\n",
      "epoch: 8, train loss: 1.756\n",
      "epoch: 8, train loss: 1.806\n",
      "epoch: 8, train loss: 1.796\n",
      "epoch: 8, train loss: 1.767\n",
      "epoch: 8, train loss: 1.777\n",
      "epoch: 8, train loss: 1.754\n",
      "epoch: 8, train loss: 1.749\n",
      "epoch: 8, train loss: 1.775\n",
      "epoch: 8, train loss: 1.750\n",
      "epoch: 8, train loss: 1.750\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fbb172f1db4e34b863118e88889fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 13500\n",
      "epoch: 8, train loss: 26.623, val loss: 1.670\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd2de33ba9e49b59085315b0585afbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, train loss: 0.123\n",
      "epoch: 9, train loss: 1.762\n",
      "epoch: 9, train loss: 1.757\n",
      "epoch: 9, train loss: 1.760\n",
      "epoch: 9, train loss: 1.774\n",
      "epoch: 9, train loss: 1.771\n",
      "epoch: 9, train loss: 1.762\n",
      "epoch: 9, train loss: 1.783\n",
      "epoch: 9, train loss: 1.778\n",
      "epoch: 9, train loss: 1.770\n",
      "epoch: 9, train loss: 1.777\n",
      "epoch: 9, train loss: 1.756\n",
      "epoch: 9, train loss: 1.756\n",
      "epoch: 9, train loss: 1.745\n",
      "epoch: 9, train loss: 1.745\n",
      "epoch: 9, train loss: 1.784\n",
      "epoch: 9, train loss: 1.756\n",
      "epoch: 9, train loss: 1.740\n",
      "epoch: 9, train loss: 1.766\n",
      "epoch: 9, train loss: 1.764\n",
      "epoch: 9, train loss: 1.765\n",
      "epoch: 9, train loss: 1.743\n",
      "epoch: 9, train loss: 1.715\n",
      "epoch: 9, train loss: 1.776\n",
      "epoch: 9, train loss: 1.766\n",
      "epoch: 9, train loss: 1.755\n",
      "epoch: 9, train loss: 1.758\n",
      "epoch: 9, train loss: 1.756\n",
      "epoch: 9, train loss: 1.722\n",
      "epoch: 9, train loss: 1.754\n",
      "epoch: 9, train loss: 1.742\n",
      "epoch: 9, train loss: 1.770\n",
      "epoch: 9, train loss: 1.753\n",
      "epoch: 9, train loss: 1.752\n",
      "epoch: 9, train loss: 1.789\n",
      "epoch: 9, train loss: 1.776\n",
      "epoch: 9, train loss: 1.745\n",
      "epoch: 9, train loss: 1.741\n",
      "epoch: 9, train loss: 1.760\n",
      "epoch: 9, train loss: 1.735\n",
      "epoch: 9, train loss: 1.764\n",
      "epoch: 9, train loss: 1.750\n",
      "epoch: 9, train loss: 1.709\n",
      "epoch: 9, train loss: 1.758\n",
      "epoch: 9, train loss: 1.763\n",
      "epoch: 9, train loss: 1.749\n",
      "epoch: 9, train loss: 1.742\n",
      "epoch: 9, train loss: 1.740\n",
      "epoch: 9, train loss: 1.747\n",
      "epoch: 9, train loss: 1.739\n",
      "epoch: 9, train loss: 1.732\n",
      "epoch: 9, train loss: 1.738\n",
      "epoch: 9, train loss: 1.764\n",
      "epoch: 9, train loss: 1.736\n",
      "epoch: 9, train loss: 1.746\n",
      "epoch: 9, train loss: 1.750\n",
      "epoch: 9, train loss: 1.740\n",
      "epoch: 9, train loss: 1.729\n",
      "epoch: 9, train loss: 1.763\n",
      "epoch: 9, train loss: 1.749\n",
      "epoch: 9, train loss: 1.768\n",
      "epoch: 9, train loss: 1.772\n",
      "epoch: 9, train loss: 1.738\n",
      "epoch: 9, train loss: 1.734\n",
      "epoch: 9, train loss: 1.709\n",
      "epoch: 9, train loss: 1.735\n",
      "epoch: 9, train loss: 1.747\n",
      "epoch: 9, train loss: 1.742\n",
      "epoch: 9, train loss: 1.759\n",
      "epoch: 9, train loss: 1.747\n",
      "epoch: 9, train loss: 1.746\n",
      "epoch: 9, train loss: 1.732\n",
      "epoch: 9, train loss: 1.726\n",
      "epoch: 9, train loss: 1.724\n",
      "epoch: 9, train loss: 1.716\n",
      "epoch: 9, train loss: 1.723\n",
      "epoch: 9, train loss: 1.719\n",
      "epoch: 9, train loss: 1.716\n",
      "epoch: 9, train loss: 1.718\n",
      "epoch: 9, train loss: 1.706\n",
      "epoch: 9, train loss: 1.729\n",
      "epoch: 9, train loss: 1.730\n",
      "epoch: 9, train loss: 1.739\n",
      "epoch: 9, train loss: 1.719\n",
      "epoch: 9, train loss: 1.731\n",
      "epoch: 9, train loss: 1.748\n",
      "epoch: 9, train loss: 1.721\n",
      "epoch: 9, train loss: 1.726\n",
      "epoch: 9, train loss: 1.732\n",
      "epoch: 9, train loss: 1.730\n",
      "epoch: 9, train loss: 1.718\n",
      "epoch: 9, train loss: 1.735\n",
      "epoch: 9, train loss: 1.726\n",
      "epoch: 9, train loss: 1.739\n",
      "epoch: 9, train loss: 1.708\n",
      "epoch: 9, train loss: 1.705\n",
      "epoch: 9, train loss: 1.713\n",
      "epoch: 9, train loss: 1.716\n",
      "epoch: 9, train loss: 1.730\n",
      "epoch: 9, train loss: 1.693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84ff0a5305a4f82a8019d31d59ff060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 15000\n",
      "epoch: 9, train loss: 25.355, val loss: 1.626\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5dbe58b851a4658b904f6ec3b9b6028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, train loss: 0.112\n",
      "epoch: 10, train loss: 1.704\n",
      "epoch: 10, train loss: 1.715\n",
      "epoch: 10, train loss: 1.727\n",
      "epoch: 10, train loss: 1.729\n",
      "epoch: 10, train loss: 1.730\n",
      "epoch: 10, train loss: 1.728\n",
      "epoch: 10, train loss: 1.698\n",
      "epoch: 10, train loss: 1.738\n",
      "epoch: 10, train loss: 1.706\n",
      "epoch: 10, train loss: 1.713\n",
      "epoch: 10, train loss: 1.698\n",
      "epoch: 10, train loss: 1.726\n",
      "epoch: 10, train loss: 1.713\n",
      "epoch: 10, train loss: 1.700\n",
      "epoch: 10, train loss: 1.714\n",
      "epoch: 10, train loss: 1.720\n",
      "epoch: 10, train loss: 1.695\n",
      "epoch: 10, train loss: 1.701\n",
      "epoch: 10, train loss: 1.701\n",
      "epoch: 10, train loss: 1.716\n",
      "epoch: 10, train loss: 1.705\n",
      "epoch: 10, train loss: 1.716\n",
      "epoch: 10, train loss: 1.695\n",
      "epoch: 10, train loss: 1.686\n",
      "epoch: 10, train loss: 1.731\n",
      "epoch: 10, train loss: 1.688\n",
      "epoch: 10, train loss: 1.723\n",
      "epoch: 10, train loss: 1.707\n",
      "epoch: 10, train loss: 1.705\n",
      "epoch: 10, train loss: 1.697\n",
      "epoch: 10, train loss: 1.726\n",
      "epoch: 10, train loss: 1.711\n",
      "epoch: 10, train loss: 1.683\n",
      "epoch: 10, train loss: 1.716\n",
      "epoch: 10, train loss: 1.701\n",
      "epoch: 10, train loss: 1.688\n",
      "epoch: 10, train loss: 1.702\n",
      "epoch: 10, train loss: 1.706\n",
      "epoch: 10, train loss: 1.697\n",
      "epoch: 10, train loss: 1.691\n",
      "epoch: 10, train loss: 1.688\n",
      "epoch: 10, train loss: 1.685\n",
      "epoch: 10, train loss: 1.691\n",
      "epoch: 10, train loss: 1.700\n",
      "epoch: 10, train loss: 1.707\n",
      "epoch: 10, train loss: 1.678\n",
      "epoch: 10, train loss: 1.674\n",
      "epoch: 10, train loss: 1.678\n",
      "epoch: 10, train loss: 1.676\n",
      "epoch: 10, train loss: 1.677\n",
      "epoch: 10, train loss: 1.677\n",
      "epoch: 10, train loss: 1.695\n",
      "epoch: 10, train loss: 1.700\n",
      "epoch: 10, train loss: 1.670\n",
      "epoch: 10, train loss: 1.677\n",
      "epoch: 10, train loss: 1.698\n",
      "epoch: 10, train loss: 1.699\n",
      "epoch: 10, train loss: 1.679\n",
      "epoch: 10, train loss: 1.695\n",
      "epoch: 10, train loss: 1.684\n",
      "epoch: 10, train loss: 1.692\n",
      "epoch: 10, train loss: 1.732\n",
      "epoch: 10, train loss: 1.713\n",
      "epoch: 10, train loss: 1.693\n",
      "epoch: 10, train loss: 1.695\n",
      "epoch: 10, train loss: 1.654\n",
      "epoch: 10, train loss: 1.668\n",
      "epoch: 10, train loss: 1.663\n",
      "epoch: 10, train loss: 1.690\n",
      "epoch: 10, train loss: 1.699\n",
      "epoch: 10, train loss: 1.684\n",
      "epoch: 10, train loss: 1.706\n",
      "epoch: 10, train loss: 1.693\n",
      "epoch: 10, train loss: 1.698\n",
      "epoch: 10, train loss: 1.687\n",
      "epoch: 10, train loss: 1.684\n",
      "epoch: 10, train loss: 1.681\n",
      "epoch: 10, train loss: 1.703\n",
      "epoch: 10, train loss: 1.689\n",
      "epoch: 10, train loss: 1.675\n",
      "epoch: 10, train loss: 1.681\n",
      "epoch: 10, train loss: 1.689\n",
      "epoch: 10, train loss: 1.670\n",
      "epoch: 10, train loss: 1.709\n",
      "epoch: 10, train loss: 1.676\n",
      "epoch: 10, train loss: 1.711\n",
      "epoch: 10, train loss: 1.702\n",
      "epoch: 10, train loss: 1.654\n",
      "epoch: 10, train loss: 1.669\n",
      "epoch: 10, train loss: 1.672\n",
      "epoch: 10, train loss: 1.731\n",
      "epoch: 10, train loss: 1.655\n",
      "epoch: 10, train loss: 1.668\n",
      "epoch: 10, train loss: 1.676\n",
      "epoch: 10, train loss: 1.657\n",
      "epoch: 10, train loss: 1.692\n",
      "epoch: 10, train loss: 1.700\n",
      "epoch: 10, train loss: 1.683\n",
      "epoch: 10, train loss: 1.717\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324f3127a15b4bd08846fc9a8132100a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 16500\n",
      "epoch: 10, train loss: 25.270, val loss: 1.583\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497fc1d4bf684117881575b243160a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11, train loss: 0.116\n",
      "epoch: 11, train loss: 1.675\n",
      "epoch: 11, train loss: 1.655\n",
      "epoch: 11, train loss: 1.682\n",
      "epoch: 11, train loss: 1.665\n",
      "epoch: 11, train loss: 1.699\n",
      "epoch: 11, train loss: 1.649\n",
      "epoch: 11, train loss: 1.633\n",
      "epoch: 11, train loss: 1.675\n",
      "epoch: 11, train loss: 1.708\n",
      "epoch: 11, train loss: 1.675\n",
      "epoch: 11, train loss: 1.668\n",
      "epoch: 11, train loss: 1.689\n",
      "epoch: 11, train loss: 1.680\n",
      "epoch: 11, train loss: 1.650\n",
      "epoch: 11, train loss: 1.684\n",
      "epoch: 11, train loss: 1.655\n",
      "epoch: 11, train loss: 1.666\n",
      "epoch: 11, train loss: 1.650\n",
      "epoch: 11, train loss: 1.662\n",
      "epoch: 11, train loss: 1.647\n",
      "epoch: 11, train loss: 1.697\n",
      "epoch: 11, train loss: 1.658\n",
      "epoch: 11, train loss: 1.652\n",
      "epoch: 11, train loss: 1.679\n",
      "epoch: 11, train loss: 1.689\n",
      "epoch: 11, train loss: 1.683\n",
      "epoch: 11, train loss: 1.673\n",
      "epoch: 11, train loss: 1.646\n",
      "epoch: 11, train loss: 1.659\n",
      "epoch: 11, train loss: 1.642\n",
      "epoch: 11, train loss: 1.674\n",
      "epoch: 11, train loss: 1.643\n",
      "epoch: 11, train loss: 1.655\n",
      "epoch: 11, train loss: 1.664\n",
      "epoch: 11, train loss: 1.666\n",
      "epoch: 11, train loss: 1.671\n",
      "epoch: 11, train loss: 1.673\n",
      "epoch: 11, train loss: 1.664\n",
      "epoch: 11, train loss: 1.666\n",
      "epoch: 11, train loss: 1.678\n",
      "epoch: 11, train loss: 1.644\n",
      "epoch: 11, train loss: 1.654\n",
      "epoch: 11, train loss: 1.641\n",
      "epoch: 11, train loss: 1.648\n",
      "epoch: 11, train loss: 1.641\n",
      "epoch: 11, train loss: 1.656\n",
      "epoch: 11, train loss: 1.667\n",
      "epoch: 11, train loss: 1.668\n",
      "epoch: 11, train loss: 1.685\n",
      "epoch: 11, train loss: 1.661\n",
      "epoch: 11, train loss: 1.661\n",
      "epoch: 11, train loss: 1.659\n",
      "epoch: 11, train loss: 1.654\n",
      "epoch: 11, train loss: 1.660\n",
      "epoch: 11, train loss: 1.653\n",
      "epoch: 11, train loss: 1.667\n",
      "epoch: 11, train loss: 1.665\n",
      "epoch: 11, train loss: 1.661\n",
      "epoch: 11, train loss: 1.678\n",
      "epoch: 11, train loss: 1.671\n",
      "epoch: 11, train loss: 1.667\n",
      "epoch: 11, train loss: 1.662\n",
      "epoch: 11, train loss: 1.644\n",
      "epoch: 11, train loss: 1.667\n",
      "epoch: 11, train loss: 1.653\n",
      "epoch: 11, train loss: 1.636\n",
      "epoch: 11, train loss: 1.687\n",
      "epoch: 11, train loss: 1.655\n",
      "epoch: 11, train loss: 1.656\n",
      "epoch: 11, train loss: 1.675\n",
      "epoch: 11, train loss: 1.653\n",
      "epoch: 11, train loss: 1.656\n",
      "epoch: 11, train loss: 1.615\n",
      "epoch: 11, train loss: 1.662\n",
      "epoch: 11, train loss: 1.643\n",
      "epoch: 11, train loss: 1.659\n",
      "epoch: 11, train loss: 1.661\n",
      "epoch: 11, train loss: 1.639\n",
      "epoch: 11, train loss: 1.621\n",
      "epoch: 11, train loss: 1.655\n",
      "epoch: 11, train loss: 1.633\n",
      "epoch: 11, train loss: 1.685\n",
      "epoch: 11, train loss: 1.655\n",
      "epoch: 11, train loss: 1.653\n",
      "epoch: 11, train loss: 1.647\n",
      "epoch: 11, train loss: 1.631\n",
      "epoch: 11, train loss: 1.626\n",
      "epoch: 11, train loss: 1.662\n",
      "epoch: 11, train loss: 1.657\n",
      "epoch: 11, train loss: 1.633\n",
      "epoch: 11, train loss: 1.645\n",
      "epoch: 11, train loss: 1.635\n",
      "epoch: 11, train loss: 1.650\n",
      "epoch: 11, train loss: 1.650\n",
      "epoch: 11, train loss: 1.656\n",
      "epoch: 11, train loss: 1.669\n",
      "epoch: 11, train loss: 1.662\n",
      "epoch: 11, train loss: 1.633\n",
      "epoch: 11, train loss: 1.649\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388b9beea246459c8d4f7355d2968d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 18000\n",
      "epoch: 11, train loss: 24.631, val loss: 1.551\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbe853a99224af7ba8dd7ad72751411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, train loss: 0.116\n",
      "epoch: 12, train loss: 1.649\n",
      "epoch: 12, train loss: 1.652\n",
      "epoch: 12, train loss: 1.638\n",
      "epoch: 12, train loss: 1.652\n",
      "epoch: 12, train loss: 1.641\n",
      "epoch: 12, train loss: 1.643\n",
      "epoch: 12, train loss: 1.645\n",
      "epoch: 12, train loss: 1.624\n",
      "epoch: 12, train loss: 1.647\n",
      "epoch: 12, train loss: 1.630\n",
      "epoch: 12, train loss: 1.630\n",
      "epoch: 12, train loss: 1.661\n",
      "epoch: 12, train loss: 1.636\n",
      "epoch: 12, train loss: 1.644\n",
      "epoch: 12, train loss: 1.637\n",
      "epoch: 12, train loss: 1.636\n",
      "epoch: 12, train loss: 1.620\n",
      "epoch: 12, train loss: 1.618\n",
      "epoch: 12, train loss: 1.620\n",
      "epoch: 12, train loss: 1.630\n",
      "epoch: 12, train loss: 1.630\n",
      "epoch: 12, train loss: 1.657\n",
      "epoch: 12, train loss: 1.596\n",
      "epoch: 12, train loss: 1.651\n",
      "epoch: 12, train loss: 1.642\n",
      "epoch: 12, train loss: 1.639\n",
      "epoch: 12, train loss: 1.605\n",
      "epoch: 12, train loss: 1.625\n",
      "epoch: 12, train loss: 1.643\n",
      "epoch: 12, train loss: 1.630\n",
      "epoch: 12, train loss: 1.630\n",
      "epoch: 12, train loss: 1.627\n",
      "epoch: 12, train loss: 1.619\n",
      "epoch: 12, train loss: 1.630\n",
      "epoch: 12, train loss: 1.648\n",
      "epoch: 12, train loss: 1.637\n",
      "epoch: 12, train loss: 1.624\n",
      "epoch: 12, train loss: 1.640\n",
      "epoch: 12, train loss: 1.631\n",
      "epoch: 12, train loss: 1.631\n",
      "epoch: 12, train loss: 1.621\n",
      "epoch: 12, train loss: 1.666\n",
      "epoch: 12, train loss: 1.626\n",
      "epoch: 12, train loss: 1.633\n",
      "epoch: 12, train loss: 1.617\n",
      "epoch: 12, train loss: 1.642\n",
      "epoch: 12, train loss: 1.617\n",
      "epoch: 12, train loss: 1.630\n",
      "epoch: 12, train loss: 1.615\n",
      "epoch: 12, train loss: 1.639\n",
      "epoch: 12, train loss: 1.631\n",
      "epoch: 12, train loss: 1.648\n",
      "epoch: 12, train loss: 1.633\n",
      "epoch: 12, train loss: 1.601\n",
      "epoch: 12, train loss: 1.629\n",
      "epoch: 12, train loss: 1.618\n",
      "epoch: 12, train loss: 1.635\n",
      "epoch: 12, train loss: 1.620\n",
      "epoch: 12, train loss: 1.632\n",
      "epoch: 12, train loss: 1.634\n",
      "epoch: 12, train loss: 1.618\n",
      "epoch: 12, train loss: 1.641\n",
      "epoch: 12, train loss: 1.644\n",
      "epoch: 12, train loss: 1.604\n",
      "epoch: 12, train loss: 1.623\n",
      "epoch: 12, train loss: 1.625\n",
      "epoch: 12, train loss: 1.621\n",
      "epoch: 12, train loss: 1.628\n",
      "epoch: 12, train loss: 1.634\n",
      "epoch: 12, train loss: 1.621\n",
      "epoch: 12, train loss: 1.649\n",
      "epoch: 12, train loss: 1.603\n",
      "epoch: 12, train loss: 1.627\n",
      "epoch: 12, train loss: 1.630\n",
      "epoch: 12, train loss: 1.602\n",
      "epoch: 12, train loss: 1.602\n",
      "epoch: 12, train loss: 1.622\n",
      "epoch: 12, train loss: 1.601\n",
      "epoch: 12, train loss: 1.639\n",
      "epoch: 12, train loss: 1.612\n",
      "epoch: 12, train loss: 1.641\n",
      "epoch: 12, train loss: 1.629\n",
      "epoch: 12, train loss: 1.613\n",
      "epoch: 12, train loss: 1.629\n",
      "epoch: 12, train loss: 1.607\n",
      "epoch: 12, train loss: 1.593\n",
      "epoch: 12, train loss: 1.633\n",
      "epoch: 12, train loss: 1.616\n",
      "epoch: 12, train loss: 1.618\n",
      "epoch: 12, train loss: 1.630\n",
      "epoch: 12, train loss: 1.607\n",
      "epoch: 12, train loss: 1.601\n",
      "epoch: 12, train loss: 1.604\n",
      "epoch: 12, train loss: 1.630\n",
      "epoch: 12, train loss: 1.618\n",
      "epoch: 12, train loss: 1.620\n",
      "epoch: 12, train loss: 1.592\n",
      "epoch: 12, train loss: 1.596\n",
      "epoch: 12, train loss: 1.607\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1febce75404142dba6db1a7093459128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 19500\n",
      "epoch: 12, train loss: 23.924, val loss: 1.523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ce33191b7042259458433b0f921dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13, train loss: 0.103\n",
      "epoch: 13, train loss: 1.630\n",
      "epoch: 13, train loss: 1.600\n",
      "epoch: 13, train loss: 1.597\n",
      "epoch: 13, train loss: 1.620\n",
      "epoch: 13, train loss: 1.614\n",
      "epoch: 13, train loss: 1.627\n",
      "epoch: 13, train loss: 1.633\n",
      "epoch: 13, train loss: 1.620\n",
      "epoch: 13, train loss: 1.617\n",
      "epoch: 13, train loss: 1.611\n",
      "epoch: 13, train loss: 1.611\n",
      "epoch: 13, train loss: 1.627\n",
      "epoch: 13, train loss: 1.589\n",
      "epoch: 13, train loss: 1.610\n",
      "epoch: 13, train loss: 1.620\n",
      "epoch: 13, train loss: 1.614\n",
      "epoch: 13, train loss: 1.597\n",
      "epoch: 13, train loss: 1.613\n",
      "epoch: 13, train loss: 1.630\n",
      "epoch: 13, train loss: 1.632\n",
      "epoch: 13, train loss: 1.603\n",
      "epoch: 13, train loss: 1.637\n",
      "epoch: 13, train loss: 1.590\n",
      "epoch: 13, train loss: 1.596\n",
      "epoch: 13, train loss: 1.605\n",
      "epoch: 13, train loss: 1.618\n",
      "epoch: 13, train loss: 1.592\n",
      "epoch: 13, train loss: 1.594\n",
      "epoch: 13, train loss: 1.626\n",
      "epoch: 13, train loss: 1.599\n",
      "epoch: 13, train loss: 1.590\n",
      "epoch: 13, train loss: 1.596\n",
      "epoch: 13, train loss: 1.594\n",
      "epoch: 13, train loss: 1.607\n",
      "epoch: 13, train loss: 1.575\n",
      "epoch: 13, train loss: 1.613\n",
      "epoch: 13, train loss: 1.629\n",
      "epoch: 13, train loss: 1.610\n",
      "epoch: 13, train loss: 1.595\n",
      "epoch: 13, train loss: 1.598\n",
      "epoch: 13, train loss: 1.619\n",
      "epoch: 13, train loss: 1.610\n",
      "epoch: 13, train loss: 1.601\n",
      "epoch: 13, train loss: 1.619\n",
      "epoch: 13, train loss: 1.614\n",
      "epoch: 13, train loss: 1.593\n",
      "epoch: 13, train loss: 1.624\n",
      "epoch: 13, train loss: 1.610\n",
      "epoch: 13, train loss: 1.599\n",
      "epoch: 13, train loss: 1.588\n",
      "epoch: 13, train loss: 1.573\n",
      "epoch: 13, train loss: 1.590\n",
      "epoch: 13, train loss: 1.608\n",
      "epoch: 13, train loss: 1.581\n",
      "epoch: 13, train loss: 1.605\n",
      "epoch: 13, train loss: 1.589\n",
      "epoch: 13, train loss: 1.613\n",
      "epoch: 13, train loss: 1.623\n",
      "epoch: 13, train loss: 1.625\n",
      "epoch: 13, train loss: 1.587\n",
      "epoch: 13, train loss: 1.616\n",
      "epoch: 13, train loss: 1.600\n",
      "epoch: 13, train loss: 1.616\n",
      "epoch: 13, train loss: 1.570\n",
      "epoch: 13, train loss: 1.607\n",
      "epoch: 13, train loss: 1.585\n",
      "epoch: 13, train loss: 1.618\n",
      "epoch: 13, train loss: 1.595\n",
      "epoch: 13, train loss: 1.553\n",
      "epoch: 13, train loss: 1.613\n",
      "epoch: 13, train loss: 1.602\n",
      "epoch: 13, train loss: 1.582\n",
      "epoch: 13, train loss: 1.601\n",
      "epoch: 13, train loss: 1.583\n",
      "epoch: 13, train loss: 1.620\n",
      "epoch: 13, train loss: 1.610\n",
      "epoch: 13, train loss: 1.587\n",
      "epoch: 13, train loss: 1.585\n",
      "epoch: 13, train loss: 1.595\n",
      "epoch: 13, train loss: 1.586\n",
      "epoch: 13, train loss: 1.590\n",
      "epoch: 13, train loss: 1.616\n",
      "epoch: 13, train loss: 1.603\n",
      "epoch: 13, train loss: 1.617\n",
      "epoch: 13, train loss: 1.607\n",
      "epoch: 13, train loss: 1.566\n",
      "epoch: 13, train loss: 1.595\n",
      "epoch: 13, train loss: 1.611\n",
      "epoch: 13, train loss: 1.590\n",
      "epoch: 13, train loss: 1.573\n",
      "epoch: 13, train loss: 1.622\n",
      "epoch: 13, train loss: 1.593\n",
      "epoch: 13, train loss: 1.582\n",
      "epoch: 13, train loss: 1.610\n",
      "epoch: 13, train loss: 1.585\n",
      "epoch: 13, train loss: 1.591\n",
      "epoch: 13, train loss: 1.629\n",
      "epoch: 13, train loss: 1.613\n",
      "epoch: 13, train loss: 1.603\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35cdad686128416abc248498c2d5802a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 21000\n",
      "epoch: 13, train loss: 23.986, val loss: 1.501\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0840c8501aa429cac915b37ceeb3a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, train loss: 0.103\n",
      "epoch: 14, train loss: 1.573\n",
      "epoch: 14, train loss: 1.569\n",
      "epoch: 14, train loss: 1.591\n",
      "epoch: 14, train loss: 1.582\n",
      "epoch: 14, train loss: 1.592\n",
      "epoch: 14, train loss: 1.591\n",
      "epoch: 14, train loss: 1.601\n",
      "epoch: 14, train loss: 1.589\n",
      "epoch: 14, train loss: 1.611\n",
      "epoch: 14, train loss: 1.573\n",
      "epoch: 14, train loss: 1.586\n",
      "epoch: 14, train loss: 1.589\n",
      "epoch: 14, train loss: 1.604\n",
      "epoch: 14, train loss: 1.611\n",
      "epoch: 14, train loss: 1.600\n",
      "epoch: 14, train loss: 1.577\n",
      "epoch: 14, train loss: 1.573\n",
      "epoch: 14, train loss: 1.587\n",
      "epoch: 14, train loss: 1.582\n",
      "epoch: 14, train loss: 1.611\n",
      "epoch: 14, train loss: 1.626\n",
      "epoch: 14, train loss: 1.568\n",
      "epoch: 14, train loss: 1.587\n",
      "epoch: 14, train loss: 1.580\n",
      "epoch: 14, train loss: 1.582\n",
      "epoch: 14, train loss: 1.619\n",
      "epoch: 14, train loss: 1.579\n",
      "epoch: 14, train loss: 1.558\n",
      "epoch: 14, train loss: 1.596\n",
      "epoch: 14, train loss: 1.565\n",
      "epoch: 14, train loss: 1.588\n",
      "epoch: 14, train loss: 1.607\n",
      "epoch: 14, train loss: 1.581\n",
      "epoch: 14, train loss: 1.575\n",
      "epoch: 14, train loss: 1.566\n",
      "epoch: 14, train loss: 1.560\n",
      "epoch: 14, train loss: 1.566\n",
      "epoch: 14, train loss: 1.576\n",
      "epoch: 14, train loss: 1.575\n",
      "epoch: 14, train loss: 1.595\n",
      "epoch: 14, train loss: 1.586\n",
      "epoch: 14, train loss: 1.571\n",
      "epoch: 14, train loss: 1.577\n",
      "epoch: 14, train loss: 1.600\n",
      "epoch: 14, train loss: 1.571\n",
      "epoch: 14, train loss: 1.576\n",
      "epoch: 14, train loss: 1.584\n",
      "epoch: 14, train loss: 1.591\n",
      "epoch: 14, train loss: 1.577\n",
      "epoch: 14, train loss: 1.566\n",
      "epoch: 14, train loss: 1.559\n",
      "epoch: 14, train loss: 1.566\n",
      "epoch: 14, train loss: 1.594\n",
      "epoch: 14, train loss: 1.557\n",
      "epoch: 14, train loss: 1.579\n",
      "epoch: 14, train loss: 1.562\n",
      "epoch: 14, train loss: 1.572\n",
      "epoch: 14, train loss: 1.563\n",
      "epoch: 14, train loss: 1.587\n",
      "epoch: 14, train loss: 1.588\n",
      "epoch: 14, train loss: 1.580\n",
      "epoch: 14, train loss: 1.563\n",
      "epoch: 14, train loss: 1.583\n",
      "epoch: 14, train loss: 1.545\n",
      "epoch: 14, train loss: 1.588\n",
      "epoch: 14, train loss: 1.563\n",
      "epoch: 14, train loss: 1.567\n",
      "epoch: 14, train loss: 1.601\n",
      "epoch: 14, train loss: 1.564\n",
      "epoch: 14, train loss: 1.558\n",
      "epoch: 14, train loss: 1.564\n",
      "epoch: 14, train loss: 1.585\n",
      "epoch: 14, train loss: 1.573\n",
      "epoch: 14, train loss: 1.553\n",
      "epoch: 14, train loss: 1.530\n",
      "epoch: 14, train loss: 1.578\n",
      "epoch: 14, train loss: 1.588\n",
      "epoch: 14, train loss: 1.595\n",
      "epoch: 14, train loss: 1.572\n",
      "epoch: 14, train loss: 1.577\n",
      "epoch: 14, train loss: 1.543\n",
      "epoch: 14, train loss: 1.581\n",
      "epoch: 14, train loss: 1.573\n",
      "epoch: 14, train loss: 1.570\n",
      "epoch: 14, train loss: 1.603\n",
      "epoch: 14, train loss: 1.591\n",
      "epoch: 14, train loss: 1.583\n",
      "epoch: 14, train loss: 1.563\n",
      "epoch: 14, train loss: 1.575\n",
      "epoch: 14, train loss: 1.571\n",
      "epoch: 14, train loss: 1.566\n",
      "epoch: 14, train loss: 1.567\n",
      "epoch: 14, train loss: 1.589\n",
      "epoch: 14, train loss: 1.573\n",
      "epoch: 14, train loss: 1.586\n",
      "epoch: 14, train loss: 1.555\n",
      "epoch: 14, train loss: 1.588\n",
      "epoch: 14, train loss: 1.560\n",
      "epoch: 14, train loss: 1.569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e079c946901e4fe6b210cc5cfcbcbab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 22500\n",
      "epoch: 14, train loss: 23.187, val loss: 1.478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9572ae265049e4833c7f27c49290df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, train loss: 0.105\n",
      "epoch: 15, train loss: 1.573\n",
      "epoch: 15, train loss: 1.544\n",
      "epoch: 15, train loss: 1.577\n",
      "epoch: 15, train loss: 1.579\n",
      "epoch: 15, train loss: 1.603\n",
      "epoch: 15, train loss: 1.573\n",
      "epoch: 15, train loss: 1.550\n",
      "epoch: 15, train loss: 1.592\n",
      "epoch: 15, train loss: 1.554\n",
      "epoch: 15, train loss: 1.562\n",
      "epoch: 15, train loss: 1.533\n",
      "epoch: 15, train loss: 1.557\n",
      "epoch: 15, train loss: 1.561\n",
      "epoch: 15, train loss: 1.587\n",
      "epoch: 15, train loss: 1.568\n",
      "epoch: 15, train loss: 1.581\n",
      "epoch: 15, train loss: 1.545\n",
      "epoch: 15, train loss: 1.549\n",
      "epoch: 15, train loss: 1.558\n",
      "epoch: 15, train loss: 1.564\n",
      "epoch: 15, train loss: 1.557\n",
      "epoch: 15, train loss: 1.561\n",
      "epoch: 15, train loss: 1.556\n",
      "epoch: 15, train loss: 1.561\n",
      "epoch: 15, train loss: 1.592\n",
      "epoch: 15, train loss: 1.556\n",
      "epoch: 15, train loss: 1.540\n",
      "epoch: 15, train loss: 1.559\n",
      "epoch: 15, train loss: 1.570\n",
      "epoch: 15, train loss: 1.575\n",
      "epoch: 15, train loss: 1.581\n",
      "epoch: 15, train loss: 1.574\n",
      "epoch: 15, train loss: 1.571\n",
      "epoch: 15, train loss: 1.551\n",
      "epoch: 15, train loss: 1.564\n",
      "epoch: 15, train loss: 1.558\n",
      "epoch: 15, train loss: 1.573\n",
      "epoch: 15, train loss: 1.547\n",
      "epoch: 15, train loss: 1.561\n",
      "epoch: 15, train loss: 1.551\n",
      "epoch: 15, train loss: 1.551\n",
      "epoch: 15, train loss: 1.536\n",
      "epoch: 15, train loss: 1.534\n",
      "epoch: 15, train loss: 1.536\n",
      "epoch: 15, train loss: 1.569\n",
      "epoch: 15, train loss: 1.560\n",
      "epoch: 15, train loss: 1.574\n",
      "epoch: 15, train loss: 1.549\n",
      "epoch: 15, train loss: 1.533\n",
      "epoch: 15, train loss: 1.585\n",
      "epoch: 15, train loss: 1.539\n",
      "epoch: 15, train loss: 1.562\n",
      "epoch: 15, train loss: 1.562\n",
      "epoch: 15, train loss: 1.592\n",
      "epoch: 15, train loss: 1.561\n",
      "epoch: 15, train loss: 1.533\n",
      "epoch: 15, train loss: 1.553\n",
      "epoch: 15, train loss: 1.521\n",
      "epoch: 15, train loss: 1.572\n",
      "epoch: 15, train loss: 1.540\n",
      "epoch: 15, train loss: 1.565\n",
      "epoch: 15, train loss: 1.518\n",
      "epoch: 15, train loss: 1.550\n",
      "epoch: 15, train loss: 1.570\n",
      "epoch: 15, train loss: 1.579\n",
      "epoch: 15, train loss: 1.547\n",
      "epoch: 15, train loss: 1.577\n",
      "epoch: 15, train loss: 1.554\n",
      "epoch: 15, train loss: 1.540\n",
      "epoch: 15, train loss: 1.529\n",
      "epoch: 15, train loss: 1.551\n",
      "epoch: 15, train loss: 1.539\n",
      "epoch: 15, train loss: 1.566\n",
      "epoch: 15, train loss: 1.547\n",
      "epoch: 15, train loss: 1.543\n",
      "epoch: 15, train loss: 1.564\n",
      "epoch: 15, train loss: 1.568\n",
      "epoch: 15, train loss: 1.555\n",
      "epoch: 15, train loss: 1.535\n",
      "epoch: 15, train loss: 1.582\n",
      "epoch: 15, train loss: 1.542\n",
      "epoch: 15, train loss: 1.534\n",
      "epoch: 15, train loss: 1.570\n",
      "epoch: 15, train loss: 1.523\n",
      "epoch: 15, train loss: 1.533\n",
      "epoch: 15, train loss: 1.576\n",
      "epoch: 15, train loss: 1.536\n",
      "epoch: 15, train loss: 1.557\n",
      "epoch: 15, train loss: 1.569\n",
      "epoch: 15, train loss: 1.568\n",
      "epoch: 15, train loss: 1.546\n",
      "epoch: 15, train loss: 1.557\n",
      "epoch: 15, train loss: 1.561\n",
      "epoch: 15, train loss: 1.551\n",
      "epoch: 15, train loss: 1.544\n",
      "epoch: 15, train loss: 1.533\n",
      "epoch: 15, train loss: 1.521\n",
      "epoch: 15, train loss: 1.551\n",
      "epoch: 15, train loss: 1.555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5994fbdb0aef446d8360e64d6d60e1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 24000\n",
      "epoch: 15, train loss: 23.136, val loss: 1.458\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8633e18cb74152b76d53659203e21e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16, train loss: 0.100\n",
      "epoch: 16, train loss: 1.549\n",
      "epoch: 16, train loss: 1.520\n",
      "epoch: 16, train loss: 1.537\n",
      "epoch: 16, train loss: 1.555\n",
      "epoch: 16, train loss: 1.555\n",
      "epoch: 16, train loss: 1.555\n",
      "epoch: 16, train loss: 1.541\n",
      "epoch: 16, train loss: 1.530\n",
      "epoch: 16, train loss: 1.528\n",
      "epoch: 16, train loss: 1.556\n",
      "epoch: 16, train loss: 1.524\n",
      "epoch: 16, train loss: 1.529\n",
      "epoch: 16, train loss: 1.537\n",
      "epoch: 16, train loss: 1.539\n",
      "epoch: 16, train loss: 1.543\n",
      "epoch: 16, train loss: 1.562\n",
      "epoch: 16, train loss: 1.536\n",
      "epoch: 16, train loss: 1.543\n",
      "epoch: 16, train loss: 1.558\n",
      "epoch: 16, train loss: 1.537\n",
      "epoch: 16, train loss: 1.540\n",
      "epoch: 16, train loss: 1.544\n",
      "epoch: 16, train loss: 1.537\n",
      "epoch: 16, train loss: 1.552\n",
      "epoch: 16, train loss: 1.550\n",
      "epoch: 16, train loss: 1.541\n",
      "epoch: 16, train loss: 1.544\n",
      "epoch: 16, train loss: 1.540\n",
      "epoch: 16, train loss: 1.540\n",
      "epoch: 16, train loss: 1.547\n",
      "epoch: 16, train loss: 1.547\n",
      "epoch: 16, train loss: 1.533\n",
      "epoch: 16, train loss: 1.536\n",
      "epoch: 16, train loss: 1.543\n",
      "epoch: 16, train loss: 1.522\n",
      "epoch: 16, train loss: 1.523\n",
      "epoch: 16, train loss: 1.564\n",
      "epoch: 16, train loss: 1.548\n",
      "epoch: 16, train loss: 1.554\n",
      "epoch: 16, train loss: 1.544\n",
      "epoch: 16, train loss: 1.541\n",
      "epoch: 16, train loss: 1.541\n",
      "epoch: 16, train loss: 1.550\n",
      "epoch: 16, train loss: 1.524\n",
      "epoch: 16, train loss: 1.533\n",
      "epoch: 16, train loss: 1.540\n",
      "epoch: 16, train loss: 1.539\n",
      "epoch: 16, train loss: 1.516\n",
      "epoch: 16, train loss: 1.540\n",
      "epoch: 16, train loss: 1.563\n",
      "epoch: 16, train loss: 1.568\n",
      "epoch: 16, train loss: 1.535\n",
      "epoch: 16, train loss: 1.546\n",
      "epoch: 16, train loss: 1.520\n",
      "epoch: 16, train loss: 1.548\n",
      "epoch: 16, train loss: 1.529\n",
      "epoch: 16, train loss: 1.523\n",
      "epoch: 16, train loss: 1.527\n",
      "epoch: 16, train loss: 1.556\n",
      "epoch: 16, train loss: 1.528\n",
      "epoch: 16, train loss: 1.520\n",
      "epoch: 16, train loss: 1.523\n",
      "epoch: 16, train loss: 1.530\n",
      "epoch: 16, train loss: 1.541\n",
      "epoch: 16, train loss: 1.510\n",
      "epoch: 16, train loss: 1.560\n",
      "epoch: 16, train loss: 1.540\n",
      "epoch: 16, train loss: 1.521\n",
      "epoch: 16, train loss: 1.554\n",
      "epoch: 16, train loss: 1.528\n",
      "epoch: 16, train loss: 1.530\n",
      "epoch: 16, train loss: 1.545\n",
      "epoch: 16, train loss: 1.529\n",
      "epoch: 16, train loss: 1.556\n",
      "epoch: 16, train loss: 1.541\n",
      "epoch: 16, train loss: 1.511\n",
      "epoch: 16, train loss: 1.555\n",
      "epoch: 16, train loss: 1.522\n",
      "epoch: 16, train loss: 1.518\n",
      "epoch: 16, train loss: 1.545\n",
      "epoch: 16, train loss: 1.554\n",
      "epoch: 16, train loss: 1.527\n",
      "epoch: 16, train loss: 1.527\n",
      "epoch: 16, train loss: 1.533\n",
      "epoch: 16, train loss: 1.550\n",
      "epoch: 16, train loss: 1.515\n",
      "epoch: 16, train loss: 1.555\n",
      "epoch: 16, train loss: 1.540\n",
      "epoch: 16, train loss: 1.530\n",
      "epoch: 16, train loss: 1.517\n",
      "epoch: 16, train loss: 1.524\n",
      "epoch: 16, train loss: 1.557\n",
      "epoch: 16, train loss: 1.542\n",
      "epoch: 16, train loss: 1.535\n",
      "epoch: 16, train loss: 1.530\n",
      "epoch: 16, train loss: 1.561\n",
      "epoch: 16, train loss: 1.517\n",
      "epoch: 16, train loss: 1.535\n",
      "epoch: 16, train loss: 1.516\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27355b0d10734d6498c4cdfc743931f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 25500\n",
      "epoch: 16, train loss: 23.160, val loss: 1.441\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49945772f1874712b6b30781ae4c85e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17, train loss: 0.103\n",
      "epoch: 17, train loss: 1.533\n",
      "epoch: 17, train loss: 1.538\n",
      "epoch: 17, train loss: 1.539\n",
      "epoch: 17, train loss: 1.544\n",
      "epoch: 17, train loss: 1.511\n",
      "epoch: 17, train loss: 1.508\n",
      "epoch: 17, train loss: 1.526\n",
      "epoch: 17, train loss: 1.537\n",
      "epoch: 17, train loss: 1.533\n",
      "epoch: 17, train loss: 1.529\n",
      "epoch: 17, train loss: 1.547\n",
      "epoch: 17, train loss: 1.522\n",
      "epoch: 17, train loss: 1.535\n",
      "epoch: 17, train loss: 1.514\n",
      "epoch: 17, train loss: 1.507\n",
      "epoch: 17, train loss: 1.530\n",
      "epoch: 17, train loss: 1.502\n",
      "epoch: 17, train loss: 1.524\n",
      "epoch: 17, train loss: 1.503\n",
      "epoch: 17, train loss: 1.514\n",
      "epoch: 17, train loss: 1.511\n",
      "epoch: 17, train loss: 1.499\n",
      "epoch: 17, train loss: 1.514\n",
      "epoch: 17, train loss: 1.524\n",
      "epoch: 17, train loss: 1.522\n",
      "epoch: 17, train loss: 1.537\n",
      "epoch: 17, train loss: 1.517\n",
      "epoch: 17, train loss: 1.519\n",
      "epoch: 17, train loss: 1.524\n",
      "epoch: 17, train loss: 1.513\n",
      "epoch: 17, train loss: 1.507\n",
      "epoch: 17, train loss: 1.519\n",
      "epoch: 17, train loss: 1.534\n",
      "epoch: 17, train loss: 1.491\n",
      "epoch: 17, train loss: 1.533\n",
      "epoch: 17, train loss: 1.522\n",
      "epoch: 17, train loss: 1.525\n",
      "epoch: 17, train loss: 1.520\n",
      "epoch: 17, train loss: 1.528\n",
      "epoch: 17, train loss: 1.545\n",
      "epoch: 17, train loss: 1.534\n",
      "epoch: 17, train loss: 1.510\n",
      "epoch: 17, train loss: 1.542\n",
      "epoch: 17, train loss: 1.535\n",
      "epoch: 17, train loss: 1.545\n",
      "epoch: 17, train loss: 1.533\n",
      "epoch: 17, train loss: 1.533\n",
      "epoch: 17, train loss: 1.534\n",
      "epoch: 17, train loss: 1.512\n",
      "epoch: 17, train loss: 1.531\n",
      "epoch: 17, train loss: 1.503\n",
      "epoch: 17, train loss: 1.521\n",
      "epoch: 17, train loss: 1.503\n",
      "epoch: 17, train loss: 1.529\n",
      "epoch: 17, train loss: 1.532\n",
      "epoch: 17, train loss: 1.522\n",
      "epoch: 17, train loss: 1.511\n",
      "epoch: 17, train loss: 1.518\n",
      "epoch: 17, train loss: 1.508\n",
      "epoch: 17, train loss: 1.481\n",
      "epoch: 17, train loss: 1.535\n",
      "epoch: 17, train loss: 1.513\n",
      "epoch: 17, train loss: 1.512\n",
      "epoch: 17, train loss: 1.509\n",
      "epoch: 17, train loss: 1.519\n",
      "epoch: 17, train loss: 1.506\n",
      "epoch: 17, train loss: 1.534\n",
      "epoch: 17, train loss: 1.514\n",
      "epoch: 17, train loss: 1.522\n",
      "epoch: 17, train loss: 1.524\n",
      "epoch: 17, train loss: 1.511\n",
      "epoch: 17, train loss: 1.504\n",
      "epoch: 17, train loss: 1.491\n",
      "epoch: 17, train loss: 1.515\n",
      "epoch: 17, train loss: 1.520\n",
      "epoch: 17, train loss: 1.541\n",
      "epoch: 17, train loss: 1.499\n",
      "epoch: 17, train loss: 1.521\n",
      "epoch: 17, train loss: 1.519\n",
      "epoch: 17, train loss: 1.529\n",
      "epoch: 17, train loss: 1.547\n",
      "epoch: 17, train loss: 1.495\n",
      "epoch: 17, train loss: 1.504\n",
      "epoch: 17, train loss: 1.533\n",
      "epoch: 17, train loss: 1.524\n",
      "epoch: 17, train loss: 1.514\n",
      "epoch: 17, train loss: 1.526\n",
      "epoch: 17, train loss: 1.524\n",
      "epoch: 17, train loss: 1.505\n",
      "epoch: 17, train loss: 1.535\n",
      "epoch: 17, train loss: 1.518\n",
      "epoch: 17, train loss: 1.505\n",
      "epoch: 17, train loss: 1.517\n",
      "epoch: 17, train loss: 1.521\n",
      "epoch: 17, train loss: 1.518\n",
      "epoch: 17, train loss: 1.507\n",
      "epoch: 17, train loss: 1.532\n",
      "epoch: 17, train loss: 1.519\n",
      "epoch: 17, train loss: 1.502\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662b62d328764bcf8e9cca198be5a28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 27000\n",
      "epoch: 17, train loss: 22.533, val loss: 1.424\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b263b7b24fc74551804f60f51f2cc8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18, train loss: 0.094\n",
      "epoch: 18, train loss: 1.528\n",
      "epoch: 18, train loss: 1.521\n",
      "epoch: 18, train loss: 1.498\n",
      "epoch: 18, train loss: 1.521\n",
      "epoch: 18, train loss: 1.479\n",
      "epoch: 18, train loss: 1.513\n",
      "epoch: 18, train loss: 1.518\n",
      "epoch: 18, train loss: 1.520\n",
      "epoch: 18, train loss: 1.513\n",
      "epoch: 18, train loss: 1.496\n",
      "epoch: 18, train loss: 1.458\n",
      "epoch: 18, train loss: 1.505\n",
      "epoch: 18, train loss: 1.521\n",
      "epoch: 18, train loss: 1.520\n",
      "epoch: 18, train loss: 1.519\n",
      "epoch: 18, train loss: 1.523\n",
      "epoch: 18, train loss: 1.528\n",
      "epoch: 18, train loss: 1.493\n",
      "epoch: 18, train loss: 1.529\n",
      "epoch: 18, train loss: 1.504\n",
      "epoch: 18, train loss: 1.508\n",
      "epoch: 18, train loss: 1.515\n",
      "epoch: 18, train loss: 1.485\n",
      "epoch: 18, train loss: 1.516\n",
      "epoch: 18, train loss: 1.534\n",
      "epoch: 18, train loss: 1.490\n",
      "epoch: 18, train loss: 1.518\n",
      "epoch: 18, train loss: 1.527\n",
      "epoch: 18, train loss: 1.489\n",
      "epoch: 18, train loss: 1.490\n",
      "epoch: 18, train loss: 1.501\n",
      "epoch: 18, train loss: 1.484\n",
      "epoch: 18, train loss: 1.494\n",
      "epoch: 18, train loss: 1.520\n",
      "epoch: 18, train loss: 1.511\n",
      "epoch: 18, train loss: 1.511\n",
      "epoch: 18, train loss: 1.508\n",
      "epoch: 18, train loss: 1.492\n",
      "epoch: 18, train loss: 1.503\n",
      "epoch: 18, train loss: 1.509\n",
      "epoch: 18, train loss: 1.497\n",
      "epoch: 18, train loss: 1.505\n",
      "epoch: 18, train loss: 1.497\n",
      "epoch: 18, train loss: 1.498\n",
      "epoch: 18, train loss: 1.497\n",
      "epoch: 18, train loss: 1.499\n",
      "epoch: 18, train loss: 1.521\n",
      "epoch: 18, train loss: 1.480\n",
      "epoch: 18, train loss: 1.496\n",
      "epoch: 18, train loss: 1.487\n",
      "epoch: 18, train loss: 1.514\n",
      "epoch: 18, train loss: 1.510\n",
      "epoch: 18, train loss: 1.521\n",
      "epoch: 18, train loss: 1.508\n",
      "epoch: 18, train loss: 1.511\n",
      "epoch: 18, train loss: 1.501\n",
      "epoch: 18, train loss: 1.514\n",
      "epoch: 18, train loss: 1.484\n",
      "epoch: 18, train loss: 1.497\n",
      "epoch: 18, train loss: 1.500\n",
      "epoch: 18, train loss: 1.506\n",
      "epoch: 18, train loss: 1.464\n",
      "epoch: 18, train loss: 1.499\n",
      "epoch: 18, train loss: 1.474\n",
      "epoch: 18, train loss: 1.509\n",
      "epoch: 18, train loss: 1.513\n",
      "epoch: 18, train loss: 1.476\n",
      "epoch: 18, train loss: 1.497\n",
      "epoch: 18, train loss: 1.495\n",
      "epoch: 18, train loss: 1.476\n",
      "epoch: 18, train loss: 1.509\n",
      "epoch: 18, train loss: 1.518\n",
      "epoch: 18, train loss: 1.491\n",
      "epoch: 18, train loss: 1.506\n",
      "epoch: 18, train loss: 1.501\n",
      "epoch: 18, train loss: 1.498\n",
      "epoch: 18, train loss: 1.523\n",
      "epoch: 18, train loss: 1.507\n",
      "epoch: 18, train loss: 1.513\n",
      "epoch: 18, train loss: 1.498\n",
      "epoch: 18, train loss: 1.499\n",
      "epoch: 18, train loss: 1.502\n",
      "epoch: 18, train loss: 1.517\n",
      "epoch: 18, train loss: 1.492\n",
      "epoch: 18, train loss: 1.487\n",
      "epoch: 18, train loss: 1.499\n",
      "epoch: 18, train loss: 1.493\n",
      "epoch: 18, train loss: 1.481\n",
      "epoch: 18, train loss: 1.490\n",
      "epoch: 18, train loss: 1.504\n",
      "epoch: 18, train loss: 1.465\n",
      "epoch: 18, train loss: 1.491\n",
      "epoch: 18, train loss: 1.509\n",
      "epoch: 18, train loss: 1.462\n",
      "epoch: 18, train loss: 1.511\n",
      "epoch: 18, train loss: 1.489\n",
      "epoch: 18, train loss: 1.496\n",
      "epoch: 18, train loss: 1.491\n",
      "epoch: 18, train loss: 1.495\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa23a602e5bf428898a3614050bfada1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 28500\n",
      "epoch: 18, train loss: 22.664, val loss: 1.407\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8365504fe7498ea0933b7714ba8f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19, train loss: 0.102\n",
      "epoch: 19, train loss: 1.478\n",
      "epoch: 19, train loss: 1.494\n",
      "epoch: 19, train loss: 1.495\n",
      "epoch: 19, train loss: 1.503\n",
      "epoch: 19, train loss: 1.489\n",
      "epoch: 19, train loss: 1.504\n",
      "epoch: 19, train loss: 1.497\n",
      "epoch: 19, train loss: 1.489\n",
      "epoch: 19, train loss: 1.479\n",
      "epoch: 19, train loss: 1.492\n",
      "epoch: 19, train loss: 1.531\n",
      "epoch: 19, train loss: 1.483\n",
      "epoch: 19, train loss: 1.479\n",
      "epoch: 19, train loss: 1.484\n",
      "epoch: 19, train loss: 1.484\n",
      "epoch: 19, train loss: 1.497\n",
      "epoch: 19, train loss: 1.461\n",
      "epoch: 19, train loss: 1.506\n",
      "epoch: 19, train loss: 1.489\n",
      "epoch: 19, train loss: 1.483\n",
      "epoch: 19, train loss: 1.468\n",
      "epoch: 19, train loss: 1.489\n",
      "epoch: 19, train loss: 1.488\n",
      "epoch: 19, train loss: 1.457\n",
      "epoch: 19, train loss: 1.486\n",
      "epoch: 19, train loss: 1.490\n",
      "epoch: 19, train loss: 1.502\n",
      "epoch: 19, train loss: 1.485\n",
      "epoch: 19, train loss: 1.510\n",
      "epoch: 19, train loss: 1.490\n",
      "epoch: 19, train loss: 1.508\n",
      "epoch: 19, train loss: 1.499\n",
      "epoch: 19, train loss: 1.498\n",
      "epoch: 19, train loss: 1.486\n",
      "epoch: 19, train loss: 1.527\n",
      "epoch: 19, train loss: 1.483\n",
      "epoch: 19, train loss: 1.483\n",
      "epoch: 19, train loss: 1.490\n",
      "epoch: 19, train loss: 1.504\n",
      "epoch: 19, train loss: 1.483\n",
      "epoch: 19, train loss: 1.509\n",
      "epoch: 19, train loss: 1.493\n",
      "epoch: 19, train loss: 1.495\n",
      "epoch: 19, train loss: 1.494\n",
      "epoch: 19, train loss: 1.495\n",
      "epoch: 19, train loss: 1.505\n",
      "epoch: 19, train loss: 1.496\n",
      "epoch: 19, train loss: 1.511\n",
      "epoch: 19, train loss: 1.480\n",
      "epoch: 19, train loss: 1.504\n",
      "epoch: 19, train loss: 1.491\n",
      "epoch: 19, train loss: 1.485\n",
      "epoch: 19, train loss: 1.500\n",
      "epoch: 19, train loss: 1.493\n",
      "epoch: 19, train loss: 1.475\n",
      "epoch: 19, train loss: 1.477\n",
      "epoch: 19, train loss: 1.468\n",
      "epoch: 19, train loss: 1.477\n",
      "epoch: 19, train loss: 1.500\n",
      "epoch: 19, train loss: 1.454\n",
      "epoch: 19, train loss: 1.483\n",
      "epoch: 19, train loss: 1.496\n",
      "epoch: 19, train loss: 1.485\n",
      "epoch: 19, train loss: 1.483\n",
      "epoch: 19, train loss: 1.481\n",
      "epoch: 19, train loss: 1.484\n",
      "epoch: 19, train loss: 1.469\n",
      "epoch: 19, train loss: 1.481\n",
      "epoch: 19, train loss: 1.498\n",
      "epoch: 19, train loss: 1.495\n",
      "epoch: 19, train loss: 1.501\n",
      "epoch: 19, train loss: 1.477\n",
      "epoch: 19, train loss: 1.509\n",
      "epoch: 19, train loss: 1.498\n",
      "epoch: 19, train loss: 1.480\n",
      "epoch: 19, train loss: 1.484\n",
      "epoch: 19, train loss: 1.472\n",
      "epoch: 19, train loss: 1.458\n",
      "epoch: 19, train loss: 1.467\n",
      "epoch: 19, train loss: 1.482\n",
      "epoch: 19, train loss: 1.488\n",
      "epoch: 19, train loss: 1.497\n",
      "epoch: 19, train loss: 1.492\n",
      "epoch: 19, train loss: 1.521\n",
      "epoch: 19, train loss: 1.513\n",
      "epoch: 19, train loss: 1.503\n",
      "epoch: 19, train loss: 1.486\n",
      "epoch: 19, train loss: 1.457\n",
      "epoch: 19, train loss: 1.494\n",
      "epoch: 19, train loss: 1.488\n",
      "epoch: 19, train loss: 1.458\n",
      "epoch: 19, train loss: 1.465\n",
      "epoch: 19, train loss: 1.463\n",
      "epoch: 19, train loss: 1.489\n",
      "epoch: 19, train loss: 1.462\n",
      "epoch: 19, train loss: 1.488\n",
      "epoch: 19, train loss: 1.490\n",
      "epoch: 19, train loss: 1.491\n",
      "epoch: 19, train loss: 1.482\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414e587f0bc14235bc0a9ef4056913cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 30000\n",
      "epoch: 19, train loss: 22.543, val loss: 1.394\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018639ef8f3b4b799785b2ce5c254ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, train loss: 0.097\n",
      "epoch: 20, train loss: 1.511\n",
      "epoch: 20, train loss: 1.490\n",
      "epoch: 20, train loss: 1.513\n",
      "epoch: 20, train loss: 1.493\n",
      "epoch: 20, train loss: 1.493\n",
      "epoch: 20, train loss: 1.499\n",
      "epoch: 20, train loss: 1.494\n",
      "epoch: 20, train loss: 1.472\n",
      "epoch: 20, train loss: 1.469\n",
      "epoch: 20, train loss: 1.491\n",
      "epoch: 20, train loss: 1.501\n",
      "epoch: 20, train loss: 1.464\n",
      "epoch: 20, train loss: 1.467\n",
      "epoch: 20, train loss: 1.498\n",
      "epoch: 20, train loss: 1.491\n",
      "epoch: 20, train loss: 1.499\n",
      "epoch: 20, train loss: 1.481\n",
      "epoch: 20, train loss: 1.475\n",
      "epoch: 20, train loss: 1.478\n",
      "epoch: 20, train loss: 1.457\n",
      "epoch: 20, train loss: 1.482\n",
      "epoch: 20, train loss: 1.501\n",
      "epoch: 20, train loss: 1.489\n",
      "epoch: 20, train loss: 1.463\n",
      "epoch: 20, train loss: 1.498\n",
      "epoch: 20, train loss: 1.505\n",
      "epoch: 20, train loss: 1.474\n",
      "epoch: 20, train loss: 1.504\n",
      "epoch: 20, train loss: 1.460\n",
      "epoch: 20, train loss: 1.491\n",
      "epoch: 20, train loss: 1.460\n",
      "epoch: 20, train loss: 1.489\n",
      "epoch: 20, train loss: 1.467\n",
      "epoch: 20, train loss: 1.480\n",
      "epoch: 20, train loss: 1.500\n",
      "epoch: 20, train loss: 1.481\n",
      "epoch: 20, train loss: 1.487\n",
      "epoch: 20, train loss: 1.486\n",
      "epoch: 20, train loss: 1.480\n",
      "epoch: 20, train loss: 1.493\n",
      "epoch: 20, train loss: 1.462\n",
      "epoch: 20, train loss: 1.437\n",
      "epoch: 20, train loss: 1.480\n",
      "epoch: 20, train loss: 1.459\n",
      "epoch: 20, train loss: 1.460\n",
      "epoch: 20, train loss: 1.438\n",
      "epoch: 20, train loss: 1.468\n",
      "epoch: 20, train loss: 1.453\n",
      "epoch: 20, train loss: 1.456\n",
      "epoch: 20, train loss: 1.460\n",
      "epoch: 20, train loss: 1.476\n",
      "epoch: 20, train loss: 1.491\n",
      "epoch: 20, train loss: 1.474\n",
      "epoch: 20, train loss: 1.496\n",
      "epoch: 20, train loss: 1.490\n",
      "epoch: 20, train loss: 1.475\n",
      "epoch: 20, train loss: 1.456\n",
      "epoch: 20, train loss: 1.475\n",
      "epoch: 20, train loss: 1.488\n",
      "epoch: 20, train loss: 1.443\n",
      "epoch: 20, train loss: 1.465\n",
      "epoch: 20, train loss: 1.457\n",
      "epoch: 20, train loss: 1.484\n",
      "epoch: 20, train loss: 1.480\n",
      "epoch: 20, train loss: 1.467\n",
      "epoch: 20, train loss: 1.483\n",
      "epoch: 20, train loss: 1.459\n",
      "epoch: 20, train loss: 1.462\n",
      "epoch: 20, train loss: 1.453\n",
      "epoch: 20, train loss: 1.474\n",
      "epoch: 20, train loss: 1.467\n",
      "epoch: 20, train loss: 1.448\n",
      "epoch: 20, train loss: 1.482\n",
      "epoch: 20, train loss: 1.469\n",
      "epoch: 20, train loss: 1.466\n",
      "epoch: 20, train loss: 1.451\n",
      "epoch: 20, train loss: 1.473\n",
      "epoch: 20, train loss: 1.470\n",
      "epoch: 20, train loss: 1.487\n",
      "epoch: 20, train loss: 1.447\n",
      "epoch: 20, train loss: 1.476\n",
      "epoch: 20, train loss: 1.480\n",
      "epoch: 20, train loss: 1.461\n",
      "epoch: 20, train loss: 1.463\n",
      "epoch: 20, train loss: 1.463\n",
      "epoch: 20, train loss: 1.487\n",
      "epoch: 20, train loss: 1.451\n",
      "epoch: 20, train loss: 1.464\n",
      "epoch: 20, train loss: 1.452\n",
      "epoch: 20, train loss: 1.488\n",
      "epoch: 20, train loss: 1.480\n",
      "epoch: 20, train loss: 1.464\n",
      "epoch: 20, train loss: 1.434\n",
      "epoch: 20, train loss: 1.442\n",
      "epoch: 20, train loss: 1.439\n",
      "epoch: 20, train loss: 1.464\n",
      "epoch: 20, train loss: 1.426\n",
      "epoch: 20, train loss: 1.434\n",
      "epoch: 20, train loss: 1.442\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448064cc774d449da61a49337c0f0dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 31500\n",
      "epoch: 20, train loss: 21.798, val loss: 1.380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90ca56ab9de40e0998624ceb9be1013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21, train loss: 0.093\n",
      "epoch: 21, train loss: 1.462\n",
      "epoch: 21, train loss: 1.479\n",
      "epoch: 21, train loss: 1.480\n",
      "epoch: 21, train loss: 1.467\n",
      "epoch: 21, train loss: 1.487\n",
      "epoch: 21, train loss: 1.447\n",
      "epoch: 21, train loss: 1.428\n",
      "epoch: 21, train loss: 1.462\n",
      "epoch: 21, train loss: 1.438\n",
      "epoch: 21, train loss: 1.462\n",
      "epoch: 21, train loss: 1.431\n",
      "epoch: 21, train loss: 1.473\n",
      "epoch: 21, train loss: 1.457\n",
      "epoch: 21, train loss: 1.455\n",
      "epoch: 21, train loss: 1.463\n",
      "epoch: 21, train loss: 1.478\n",
      "epoch: 21, train loss: 1.465\n",
      "epoch: 21, train loss: 1.472\n",
      "epoch: 21, train loss: 1.457\n",
      "epoch: 21, train loss: 1.447\n",
      "epoch: 21, train loss: 1.447\n",
      "epoch: 21, train loss: 1.443\n",
      "epoch: 21, train loss: 1.480\n",
      "epoch: 21, train loss: 1.481\n",
      "epoch: 21, train loss: 1.446\n",
      "epoch: 21, train loss: 1.461\n",
      "epoch: 21, train loss: 1.473\n",
      "epoch: 21, train loss: 1.454\n",
      "epoch: 21, train loss: 1.470\n",
      "epoch: 21, train loss: 1.462\n",
      "epoch: 21, train loss: 1.476\n",
      "epoch: 21, train loss: 1.486\n",
      "epoch: 21, train loss: 1.465\n",
      "epoch: 21, train loss: 1.446\n",
      "epoch: 21, train loss: 1.460\n",
      "epoch: 21, train loss: 1.491\n",
      "epoch: 21, train loss: 1.466\n",
      "epoch: 21, train loss: 1.466\n",
      "epoch: 21, train loss: 1.473\n",
      "epoch: 21, train loss: 1.470\n",
      "epoch: 21, train loss: 1.467\n",
      "epoch: 21, train loss: 1.480\n",
      "epoch: 21, train loss: 1.465\n",
      "epoch: 21, train loss: 1.454\n",
      "epoch: 21, train loss: 1.468\n",
      "epoch: 21, train loss: 1.489\n",
      "epoch: 21, train loss: 1.490\n",
      "epoch: 21, train loss: 1.452\n",
      "epoch: 21, train loss: 1.447\n",
      "epoch: 21, train loss: 1.455\n",
      "epoch: 21, train loss: 1.451\n",
      "epoch: 21, train loss: 1.460\n",
      "epoch: 21, train loss: 1.447\n",
      "epoch: 21, train loss: 1.455\n",
      "epoch: 21, train loss: 1.462\n",
      "epoch: 21, train loss: 1.474\n",
      "epoch: 21, train loss: 1.451\n",
      "epoch: 21, train loss: 1.450\n",
      "epoch: 21, train loss: 1.442\n",
      "epoch: 21, train loss: 1.448\n",
      "epoch: 21, train loss: 1.461\n",
      "epoch: 21, train loss: 1.462\n",
      "epoch: 21, train loss: 1.472\n",
      "epoch: 21, train loss: 1.454\n",
      "epoch: 21, train loss: 1.488\n",
      "epoch: 21, train loss: 1.460\n",
      "epoch: 21, train loss: 1.457\n",
      "epoch: 21, train loss: 1.433\n",
      "epoch: 21, train loss: 1.489\n",
      "epoch: 21, train loss: 1.456\n",
      "epoch: 21, train loss: 1.452\n",
      "epoch: 21, train loss: 1.449\n",
      "epoch: 21, train loss: 1.461\n",
      "epoch: 21, train loss: 1.456\n",
      "epoch: 21, train loss: 1.427\n",
      "epoch: 21, train loss: 1.462\n",
      "epoch: 21, train loss: 1.485\n",
      "epoch: 21, train loss: 1.463\n",
      "epoch: 21, train loss: 1.465\n",
      "epoch: 21, train loss: 1.448\n",
      "epoch: 21, train loss: 1.441\n",
      "epoch: 21, train loss: 1.461\n",
      "epoch: 21, train loss: 1.450\n",
      "epoch: 21, train loss: 1.427\n",
      "epoch: 21, train loss: 1.438\n",
      "epoch: 21, train loss: 1.437\n",
      "epoch: 21, train loss: 1.455\n",
      "epoch: 21, train loss: 1.456\n",
      "epoch: 21, train loss: 1.450\n",
      "epoch: 21, train loss: 1.471\n",
      "epoch: 21, train loss: 1.436\n",
      "epoch: 21, train loss: 1.474\n",
      "epoch: 21, train loss: 1.471\n",
      "epoch: 21, train loss: 1.464\n",
      "epoch: 21, train loss: 1.455\n",
      "epoch: 21, train loss: 1.460\n",
      "epoch: 21, train loss: 1.423\n",
      "epoch: 21, train loss: 1.450\n",
      "epoch: 21, train loss: 1.478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e3b15dda59471d954a2bca10cb4f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 33000\n",
      "epoch: 21, train loss: 21.773, val loss: 1.368\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4558a9a57445a590031b02d7707dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22, train loss: 0.099\n",
      "epoch: 22, train loss: 1.452\n",
      "epoch: 22, train loss: 1.471\n",
      "epoch: 22, train loss: 1.460\n",
      "epoch: 22, train loss: 1.442\n",
      "epoch: 22, train loss: 1.453\n",
      "epoch: 22, train loss: 1.446\n",
      "epoch: 22, train loss: 1.459\n",
      "epoch: 22, train loss: 1.440\n",
      "epoch: 22, train loss: 1.434\n",
      "epoch: 22, train loss: 1.440\n",
      "epoch: 22, train loss: 1.453\n",
      "epoch: 22, train loss: 1.445\n",
      "epoch: 22, train loss: 1.461\n",
      "epoch: 22, train loss: 1.465\n",
      "epoch: 22, train loss: 1.461\n",
      "epoch: 22, train loss: 1.452\n",
      "epoch: 22, train loss: 1.448\n",
      "epoch: 22, train loss: 1.446\n",
      "epoch: 22, train loss: 1.446\n",
      "epoch: 22, train loss: 1.461\n",
      "epoch: 22, train loss: 1.447\n",
      "epoch: 22, train loss: 1.468\n",
      "epoch: 22, train loss: 1.432\n",
      "epoch: 22, train loss: 1.449\n",
      "epoch: 22, train loss: 1.436\n",
      "epoch: 22, train loss: 1.463\n",
      "epoch: 22, train loss: 1.447\n",
      "epoch: 22, train loss: 1.431\n",
      "epoch: 22, train loss: 1.452\n",
      "epoch: 22, train loss: 1.440\n",
      "epoch: 22, train loss: 1.416\n",
      "epoch: 22, train loss: 1.443\n",
      "epoch: 22, train loss: 1.412\n",
      "epoch: 22, train loss: 1.430\n",
      "epoch: 22, train loss: 1.432\n",
      "epoch: 22, train loss: 1.433\n",
      "epoch: 22, train loss: 1.468\n",
      "epoch: 22, train loss: 1.469\n",
      "epoch: 22, train loss: 1.446\n",
      "epoch: 22, train loss: 1.478\n",
      "epoch: 22, train loss: 1.458\n",
      "epoch: 22, train loss: 1.409\n",
      "epoch: 22, train loss: 1.439\n",
      "epoch: 22, train loss: 1.469\n",
      "epoch: 22, train loss: 1.474\n",
      "epoch: 22, train loss: 1.432\n",
      "epoch: 22, train loss: 1.435\n",
      "epoch: 22, train loss: 1.455\n",
      "epoch: 22, train loss: 1.440\n",
      "epoch: 22, train loss: 1.447\n",
      "epoch: 22, train loss: 1.436\n",
      "epoch: 22, train loss: 1.445\n",
      "epoch: 22, train loss: 1.446\n",
      "epoch: 22, train loss: 1.471\n",
      "epoch: 22, train loss: 1.422\n",
      "epoch: 22, train loss: 1.433\n",
      "epoch: 22, train loss: 1.446\n",
      "epoch: 22, train loss: 1.472\n",
      "epoch: 22, train loss: 1.458\n",
      "epoch: 22, train loss: 1.450\n",
      "epoch: 22, train loss: 1.466\n",
      "epoch: 22, train loss: 1.463\n",
      "epoch: 22, train loss: 1.443\n",
      "epoch: 22, train loss: 1.447\n",
      "epoch: 22, train loss: 1.453\n",
      "epoch: 22, train loss: 1.448\n",
      "epoch: 22, train loss: 1.460\n",
      "epoch: 22, train loss: 1.443\n",
      "epoch: 22, train loss: 1.425\n",
      "epoch: 22, train loss: 1.454\n",
      "epoch: 22, train loss: 1.427\n",
      "epoch: 22, train loss: 1.441\n",
      "epoch: 22, train loss: 1.450\n",
      "epoch: 22, train loss: 1.441\n",
      "epoch: 22, train loss: 1.450\n",
      "epoch: 22, train loss: 1.456\n",
      "epoch: 22, train loss: 1.435\n",
      "epoch: 22, train loss: 1.423\n",
      "epoch: 22, train loss: 1.423\n",
      "epoch: 22, train loss: 1.458\n",
      "epoch: 22, train loss: 1.416\n",
      "epoch: 22, train loss: 1.443\n",
      "epoch: 22, train loss: 1.430\n",
      "epoch: 22, train loss: 1.472\n",
      "epoch: 22, train loss: 1.434\n",
      "epoch: 22, train loss: 1.433\n",
      "epoch: 22, train loss: 1.460\n",
      "epoch: 22, train loss: 1.443\n",
      "epoch: 22, train loss: 1.436\n",
      "epoch: 22, train loss: 1.481\n",
      "epoch: 22, train loss: 1.443\n",
      "epoch: 22, train loss: 1.446\n",
      "epoch: 22, train loss: 1.443\n",
      "epoch: 22, train loss: 1.428\n",
      "epoch: 22, train loss: 1.418\n",
      "epoch: 22, train loss: 1.451\n",
      "epoch: 22, train loss: 1.450\n",
      "epoch: 22, train loss: 1.414\n",
      "epoch: 22, train loss: 1.436\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc0da15a384462d953b1e89cfe2056e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 34500\n",
      "epoch: 22, train loss: 22.009, val loss: 1.356\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e23763d076d4039aa3feaacd0d4525f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23, train loss: 0.096\n",
      "epoch: 23, train loss: 1.456\n",
      "epoch: 23, train loss: 1.417\n",
      "epoch: 23, train loss: 1.441\n",
      "epoch: 23, train loss: 1.462\n",
      "epoch: 23, train loss: 1.423\n",
      "epoch: 23, train loss: 1.444\n",
      "epoch: 23, train loss: 1.454\n",
      "epoch: 23, train loss: 1.428\n",
      "epoch: 23, train loss: 1.429\n",
      "epoch: 23, train loss: 1.446\n",
      "epoch: 23, train loss: 1.464\n",
      "epoch: 23, train loss: 1.432\n",
      "epoch: 23, train loss: 1.441\n",
      "epoch: 23, train loss: 1.438\n",
      "epoch: 23, train loss: 1.424\n",
      "epoch: 23, train loss: 1.428\n",
      "epoch: 23, train loss: 1.456\n",
      "epoch: 23, train loss: 1.444\n",
      "epoch: 23, train loss: 1.440\n",
      "epoch: 23, train loss: 1.444\n",
      "epoch: 23, train loss: 1.416\n",
      "epoch: 23, train loss: 1.435\n",
      "epoch: 23, train loss: 1.447\n",
      "epoch: 23, train loss: 1.444\n",
      "epoch: 23, train loss: 1.418\n",
      "epoch: 23, train loss: 1.438\n",
      "epoch: 23, train loss: 1.423\n",
      "epoch: 23, train loss: 1.427\n",
      "epoch: 23, train loss: 1.441\n",
      "epoch: 23, train loss: 1.459\n",
      "epoch: 23, train loss: 1.462\n",
      "epoch: 23, train loss: 1.453\n",
      "epoch: 23, train loss: 1.426\n",
      "epoch: 23, train loss: 1.425\n",
      "epoch: 23, train loss: 1.424\n",
      "epoch: 23, train loss: 1.440\n",
      "epoch: 23, train loss: 1.420\n",
      "epoch: 23, train loss: 1.441\n",
      "epoch: 23, train loss: 1.402\n",
      "epoch: 23, train loss: 1.434\n",
      "epoch: 23, train loss: 1.433\n",
      "epoch: 23, train loss: 1.440\n",
      "epoch: 23, train loss: 1.448\n",
      "epoch: 23, train loss: 1.417\n",
      "epoch: 23, train loss: 1.429\n",
      "epoch: 23, train loss: 1.451\n",
      "epoch: 23, train loss: 1.427\n",
      "epoch: 23, train loss: 1.453\n",
      "epoch: 23, train loss: 1.442\n",
      "epoch: 23, train loss: 1.437\n",
      "epoch: 23, train loss: 1.435\n",
      "epoch: 23, train loss: 1.423\n",
      "epoch: 23, train loss: 1.422\n",
      "epoch: 23, train loss: 1.451\n",
      "epoch: 23, train loss: 1.451\n",
      "epoch: 23, train loss: 1.430\n",
      "epoch: 23, train loss: 1.431\n",
      "epoch: 23, train loss: 1.446\n",
      "epoch: 23, train loss: 1.423\n",
      "epoch: 23, train loss: 1.433\n",
      "epoch: 23, train loss: 1.419\n",
      "epoch: 23, train loss: 1.465\n",
      "epoch: 23, train loss: 1.448\n",
      "epoch: 23, train loss: 1.449\n",
      "epoch: 23, train loss: 1.431\n",
      "epoch: 23, train loss: 1.451\n",
      "epoch: 23, train loss: 1.415\n",
      "epoch: 23, train loss: 1.440\n",
      "epoch: 23, train loss: 1.426\n",
      "epoch: 23, train loss: 1.461\n",
      "epoch: 23, train loss: 1.441\n",
      "epoch: 23, train loss: 1.434\n",
      "epoch: 23, train loss: 1.433\n",
      "epoch: 23, train loss: 1.462\n",
      "epoch: 23, train loss: 1.450\n",
      "epoch: 23, train loss: 1.442\n",
      "epoch: 23, train loss: 1.438\n",
      "epoch: 23, train loss: 1.448\n",
      "epoch: 23, train loss: 1.423\n",
      "epoch: 23, train loss: 1.433\n",
      "epoch: 23, train loss: 1.397\n",
      "epoch: 23, train loss: 1.451\n",
      "epoch: 23, train loss: 1.433\n",
      "epoch: 23, train loss: 1.452\n",
      "epoch: 23, train loss: 1.430\n",
      "epoch: 23, train loss: 1.409\n",
      "epoch: 23, train loss: 1.432\n",
      "epoch: 23, train loss: 1.428\n",
      "epoch: 23, train loss: 1.455\n",
      "epoch: 23, train loss: 1.438\n",
      "epoch: 23, train loss: 1.426\n",
      "epoch: 23, train loss: 1.444\n",
      "epoch: 23, train loss: 1.407\n",
      "epoch: 23, train loss: 1.441\n",
      "epoch: 23, train loss: 1.428\n",
      "epoch: 23, train loss: 1.411\n",
      "epoch: 23, train loss: 1.443\n",
      "epoch: 23, train loss: 1.442\n",
      "epoch: 23, train loss: 1.438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb67bd6c638c4f45a00a67dc7a5e569c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 36000\n",
      "epoch: 23, train loss: 21.254, val loss: 1.345\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd8f7e76d1d453ab66f7089b0c37f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24, train loss: 0.094\n",
      "epoch: 24, train loss: 1.456\n",
      "epoch: 24, train loss: 1.433\n",
      "epoch: 24, train loss: 1.414\n",
      "epoch: 24, train loss: 1.410\n",
      "epoch: 24, train loss: 1.450\n",
      "epoch: 24, train loss: 1.446\n",
      "epoch: 24, train loss: 1.428\n",
      "epoch: 24, train loss: 1.437\n",
      "epoch: 24, train loss: 1.441\n",
      "epoch: 24, train loss: 1.454\n",
      "epoch: 24, train loss: 1.414\n",
      "epoch: 24, train loss: 1.455\n",
      "epoch: 24, train loss: 1.400\n",
      "epoch: 24, train loss: 1.442\n",
      "epoch: 24, train loss: 1.433\n",
      "epoch: 24, train loss: 1.441\n",
      "epoch: 24, train loss: 1.442\n",
      "epoch: 24, train loss: 1.439\n",
      "epoch: 24, train loss: 1.431\n",
      "epoch: 24, train loss: 1.416\n",
      "epoch: 24, train loss: 1.470\n",
      "epoch: 24, train loss: 1.437\n",
      "epoch: 24, train loss: 1.462\n",
      "epoch: 24, train loss: 1.395\n",
      "epoch: 24, train loss: 1.414\n",
      "epoch: 24, train loss: 1.417\n",
      "epoch: 24, train loss: 1.430\n",
      "epoch: 24, train loss: 1.415\n",
      "epoch: 24, train loss: 1.395\n",
      "epoch: 24, train loss: 1.435\n",
      "epoch: 24, train loss: 1.404\n",
      "epoch: 24, train loss: 1.431\n",
      "epoch: 24, train loss: 1.422\n",
      "epoch: 24, train loss: 1.418\n",
      "epoch: 24, train loss: 1.424\n",
      "epoch: 24, train loss: 1.432\n",
      "epoch: 24, train loss: 1.413\n",
      "epoch: 24, train loss: 1.417\n",
      "epoch: 24, train loss: 1.450\n",
      "epoch: 24, train loss: 1.435\n",
      "epoch: 24, train loss: 1.420\n",
      "epoch: 24, train loss: 1.450\n",
      "epoch: 24, train loss: 1.403\n",
      "epoch: 24, train loss: 1.447\n",
      "epoch: 24, train loss: 1.414\n",
      "epoch: 24, train loss: 1.435\n",
      "epoch: 24, train loss: 1.437\n",
      "epoch: 24, train loss: 1.422\n",
      "epoch: 24, train loss: 1.426\n",
      "epoch: 24, train loss: 1.430\n",
      "epoch: 24, train loss: 1.442\n",
      "epoch: 24, train loss: 1.413\n",
      "epoch: 24, train loss: 1.441\n",
      "epoch: 24, train loss: 1.414\n",
      "epoch: 24, train loss: 1.425\n",
      "epoch: 24, train loss: 1.430\n",
      "epoch: 24, train loss: 1.414\n",
      "epoch: 24, train loss: 1.417\n",
      "epoch: 24, train loss: 1.395\n",
      "epoch: 24, train loss: 1.425\n",
      "epoch: 24, train loss: 1.401\n",
      "epoch: 24, train loss: 1.432\n",
      "epoch: 24, train loss: 1.458\n",
      "epoch: 24, train loss: 1.434\n",
      "epoch: 24, train loss: 1.435\n",
      "epoch: 24, train loss: 1.395\n",
      "epoch: 24, train loss: 1.412\n",
      "epoch: 24, train loss: 1.425\n",
      "epoch: 24, train loss: 1.417\n",
      "epoch: 24, train loss: 1.419\n",
      "epoch: 24, train loss: 1.412\n",
      "epoch: 24, train loss: 1.429\n",
      "epoch: 24, train loss: 1.432\n",
      "epoch: 24, train loss: 1.405\n",
      "epoch: 24, train loss: 1.426\n",
      "epoch: 24, train loss: 1.422\n",
      "epoch: 24, train loss: 1.416\n",
      "epoch: 24, train loss: 1.411\n",
      "epoch: 24, train loss: 1.420\n",
      "epoch: 24, train loss: 1.421\n",
      "epoch: 24, train loss: 1.419\n",
      "epoch: 24, train loss: 1.414\n",
      "epoch: 24, train loss: 1.441\n",
      "epoch: 24, train loss: 1.410\n",
      "epoch: 24, train loss: 1.431\n",
      "epoch: 24, train loss: 1.438\n",
      "epoch: 24, train loss: 1.436\n",
      "epoch: 24, train loss: 1.419\n",
      "epoch: 24, train loss: 1.395\n",
      "epoch: 24, train loss: 1.413\n",
      "epoch: 24, train loss: 1.415\n",
      "epoch: 24, train loss: 1.415\n",
      "epoch: 24, train loss: 1.415\n",
      "epoch: 24, train loss: 1.435\n",
      "epoch: 24, train loss: 1.412\n",
      "epoch: 24, train loss: 1.424\n",
      "epoch: 24, train loss: 1.414\n",
      "epoch: 24, train loss: 1.414\n",
      "epoch: 24, train loss: 1.415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bd1db726ab416da2f91c40ef0c1ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 37500\n",
      "epoch: 24, train loss: 21.586, val loss: 1.336\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a63a952b1049358f13150d8d317f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25, train loss: 0.096\n",
      "epoch: 25, train loss: 1.437\n",
      "epoch: 25, train loss: 1.423\n",
      "epoch: 25, train loss: 1.445\n",
      "epoch: 25, train loss: 1.425\n",
      "epoch: 25, train loss: 1.433\n",
      "epoch: 25, train loss: 1.392\n",
      "epoch: 25, train loss: 1.448\n",
      "epoch: 25, train loss: 1.409\n",
      "epoch: 25, train loss: 1.436\n",
      "epoch: 25, train loss: 1.437\n",
      "epoch: 25, train loss: 1.456\n",
      "epoch: 25, train loss: 1.397\n",
      "epoch: 25, train loss: 1.422\n",
      "epoch: 25, train loss: 1.416\n",
      "epoch: 25, train loss: 1.407\n",
      "epoch: 25, train loss: 1.423\n",
      "epoch: 25, train loss: 1.413\n",
      "epoch: 25, train loss: 1.403\n",
      "epoch: 25, train loss: 1.401\n",
      "epoch: 25, train loss: 1.420\n",
      "epoch: 25, train loss: 1.418\n",
      "epoch: 25, train loss: 1.409\n",
      "epoch: 25, train loss: 1.429\n",
      "epoch: 25, train loss: 1.426\n",
      "epoch: 25, train loss: 1.454\n",
      "epoch: 25, train loss: 1.427\n",
      "epoch: 25, train loss: 1.393\n",
      "epoch: 25, train loss: 1.426\n",
      "epoch: 25, train loss: 1.409\n",
      "epoch: 25, train loss: 1.435\n",
      "epoch: 25, train loss: 1.423\n",
      "epoch: 25, train loss: 1.428\n",
      "epoch: 25, train loss: 1.439\n",
      "epoch: 25, train loss: 1.403\n",
      "epoch: 25, train loss: 1.415\n",
      "epoch: 25, train loss: 1.414\n",
      "epoch: 25, train loss: 1.427\n",
      "epoch: 25, train loss: 1.438\n",
      "epoch: 25, train loss: 1.407\n",
      "epoch: 25, train loss: 1.406\n",
      "epoch: 25, train loss: 1.426\n",
      "epoch: 25, train loss: 1.399\n",
      "epoch: 25, train loss: 1.391\n",
      "epoch: 25, train loss: 1.420\n",
      "epoch: 25, train loss: 1.424\n",
      "epoch: 25, train loss: 1.398\n",
      "epoch: 25, train loss: 1.429\n",
      "epoch: 25, train loss: 1.438\n",
      "epoch: 25, train loss: 1.404\n",
      "epoch: 25, train loss: 1.430\n",
      "epoch: 25, train loss: 1.440\n",
      "epoch: 25, train loss: 1.419\n",
      "epoch: 25, train loss: 1.431\n",
      "epoch: 25, train loss: 1.411\n",
      "epoch: 25, train loss: 1.411\n",
      "epoch: 25, train loss: 1.441\n",
      "epoch: 25, train loss: 1.399\n",
      "epoch: 25, train loss: 1.402\n",
      "epoch: 25, train loss: 1.429\n",
      "epoch: 25, train loss: 1.408\n",
      "epoch: 25, train loss: 1.433\n",
      "epoch: 25, train loss: 1.413\n",
      "epoch: 25, train loss: 1.420\n",
      "epoch: 25, train loss: 1.414\n",
      "epoch: 25, train loss: 1.411\n",
      "epoch: 25, train loss: 1.405\n",
      "epoch: 25, train loss: 1.426\n",
      "epoch: 25, train loss: 1.416\n",
      "epoch: 25, train loss: 1.410\n",
      "epoch: 25, train loss: 1.423\n",
      "epoch: 25, train loss: 1.437\n",
      "epoch: 25, train loss: 1.442\n",
      "epoch: 25, train loss: 1.435\n",
      "epoch: 25, train loss: 1.404\n",
      "epoch: 25, train loss: 1.419\n",
      "epoch: 25, train loss: 1.401\n",
      "epoch: 25, train loss: 1.411\n",
      "epoch: 25, train loss: 1.417\n",
      "epoch: 25, train loss: 1.390\n",
      "epoch: 25, train loss: 1.419\n",
      "epoch: 25, train loss: 1.401\n",
      "epoch: 25, train loss: 1.402\n",
      "epoch: 25, train loss: 1.422\n",
      "epoch: 25, train loss: 1.426\n",
      "epoch: 25, train loss: 1.445\n",
      "epoch: 25, train loss: 1.422\n",
      "epoch: 25, train loss: 1.409\n",
      "epoch: 25, train loss: 1.438\n",
      "epoch: 25, train loss: 1.407\n",
      "epoch: 25, train loss: 1.411\n",
      "epoch: 25, train loss: 1.419\n",
      "epoch: 25, train loss: 1.416\n",
      "epoch: 25, train loss: 1.399\n",
      "epoch: 25, train loss: 1.432\n",
      "epoch: 25, train loss: 1.425\n",
      "epoch: 25, train loss: 1.400\n",
      "epoch: 25, train loss: 1.423\n",
      "epoch: 25, train loss: 1.409\n",
      "epoch: 25, train loss: 1.413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed60a83935f048179fc198f8c3d7dd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 39000\n",
      "epoch: 25, train loss: 21.113, val loss: 1.328\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9948d5cd726b46e0a40915fca7d09f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26, train loss: 0.097\n",
      "epoch: 26, train loss: 1.420\n",
      "epoch: 26, train loss: 1.414\n",
      "epoch: 26, train loss: 1.421\n",
      "epoch: 26, train loss: 1.390\n",
      "epoch: 26, train loss: 1.393\n",
      "epoch: 26, train loss: 1.417\n",
      "epoch: 26, train loss: 1.428\n",
      "epoch: 26, train loss: 1.416\n",
      "epoch: 26, train loss: 1.389\n",
      "epoch: 26, train loss: 1.400\n",
      "epoch: 26, train loss: 1.424\n",
      "epoch: 26, train loss: 1.397\n",
      "epoch: 26, train loss: 1.435\n",
      "epoch: 26, train loss: 1.402\n",
      "epoch: 26, train loss: 1.399\n",
      "epoch: 26, train loss: 1.398\n",
      "epoch: 26, train loss: 1.418\n",
      "epoch: 26, train loss: 1.426\n",
      "epoch: 26, train loss: 1.404\n",
      "epoch: 26, train loss: 1.419\n",
      "epoch: 26, train loss: 1.426\n",
      "epoch: 26, train loss: 1.399\n",
      "epoch: 26, train loss: 1.433\n",
      "epoch: 26, train loss: 1.408\n",
      "epoch: 26, train loss: 1.406\n",
      "epoch: 26, train loss: 1.403\n",
      "epoch: 26, train loss: 1.413\n",
      "epoch: 26, train loss: 1.406\n",
      "epoch: 26, train loss: 1.433\n",
      "epoch: 26, train loss: 1.427\n",
      "epoch: 26, train loss: 1.413\n",
      "epoch: 26, train loss: 1.422\n",
      "epoch: 26, train loss: 1.391\n",
      "epoch: 26, train loss: 1.418\n",
      "epoch: 26, train loss: 1.388\n",
      "epoch: 26, train loss: 1.392\n",
      "epoch: 26, train loss: 1.405\n",
      "epoch: 26, train loss: 1.411\n",
      "epoch: 26, train loss: 1.409\n",
      "epoch: 26, train loss: 1.419\n",
      "epoch: 26, train loss: 1.409\n",
      "epoch: 26, train loss: 1.417\n",
      "epoch: 26, train loss: 1.409\n",
      "epoch: 26, train loss: 1.416\n",
      "epoch: 26, train loss: 1.397\n",
      "epoch: 26, train loss: 1.400\n",
      "epoch: 26, train loss: 1.412\n",
      "epoch: 26, train loss: 1.405\n",
      "epoch: 26, train loss: 1.421\n",
      "epoch: 26, train loss: 1.400\n",
      "epoch: 26, train loss: 1.419\n",
      "epoch: 26, train loss: 1.402\n",
      "epoch: 26, train loss: 1.416\n",
      "epoch: 26, train loss: 1.410\n",
      "epoch: 26, train loss: 1.414\n",
      "epoch: 26, train loss: 1.398\n",
      "epoch: 26, train loss: 1.406\n",
      "epoch: 26, train loss: 1.415\n",
      "epoch: 26, train loss: 1.388\n",
      "epoch: 26, train loss: 1.410\n",
      "epoch: 26, train loss: 1.373\n",
      "epoch: 26, train loss: 1.393\n",
      "epoch: 26, train loss: 1.394\n",
      "epoch: 26, train loss: 1.420\n",
      "epoch: 26, train loss: 1.404\n",
      "epoch: 26, train loss: 1.403\n",
      "epoch: 26, train loss: 1.391\n",
      "epoch: 26, train loss: 1.419\n",
      "epoch: 26, train loss: 1.411\n",
      "epoch: 26, train loss: 1.415\n",
      "epoch: 26, train loss: 1.396\n",
      "epoch: 26, train loss: 1.394\n",
      "epoch: 26, train loss: 1.427\n",
      "epoch: 26, train loss: 1.394\n",
      "epoch: 26, train loss: 1.427\n",
      "epoch: 26, train loss: 1.409\n",
      "epoch: 26, train loss: 1.408\n",
      "epoch: 26, train loss: 1.411\n",
      "epoch: 26, train loss: 1.409\n",
      "epoch: 26, train loss: 1.405\n",
      "epoch: 26, train loss: 1.423\n",
      "epoch: 26, train loss: 1.423\n",
      "epoch: 26, train loss: 1.391\n",
      "epoch: 26, train loss: 1.418\n",
      "epoch: 26, train loss: 1.391\n",
      "epoch: 26, train loss: 1.415\n",
      "epoch: 26, train loss: 1.403\n",
      "epoch: 26, train loss: 1.381\n",
      "epoch: 26, train loss: 1.398\n",
      "epoch: 26, train loss: 1.400\n",
      "epoch: 26, train loss: 1.425\n",
      "epoch: 26, train loss: 1.431\n",
      "epoch: 26, train loss: 1.390\n",
      "epoch: 26, train loss: 1.428\n",
      "epoch: 26, train loss: 1.397\n",
      "epoch: 26, train loss: 1.407\n",
      "epoch: 26, train loss: 1.402\n",
      "epoch: 26, train loss: 1.424\n",
      "epoch: 26, train loss: 1.421\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39175f44663a4495aa724fd59d2f670e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 40500\n",
      "epoch: 26, train loss: 21.097, val loss: 1.321\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade51ae226704b02b55a5dc66d043c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27, train loss: 0.089\n",
      "epoch: 27, train loss: 1.409\n",
      "epoch: 27, train loss: 1.405\n",
      "epoch: 27, train loss: 1.427\n",
      "epoch: 27, train loss: 1.399\n",
      "epoch: 27, train loss: 1.411\n",
      "epoch: 27, train loss: 1.398\n",
      "epoch: 27, train loss: 1.426\n",
      "epoch: 27, train loss: 1.406\n",
      "epoch: 27, train loss: 1.397\n",
      "epoch: 27, train loss: 1.421\n",
      "epoch: 27, train loss: 1.427\n",
      "epoch: 27, train loss: 1.406\n",
      "epoch: 27, train loss: 1.414\n",
      "epoch: 27, train loss: 1.406\n",
      "epoch: 27, train loss: 1.415\n",
      "epoch: 27, train loss: 1.404\n",
      "epoch: 27, train loss: 1.401\n",
      "epoch: 27, train loss: 1.406\n",
      "epoch: 27, train loss: 1.418\n",
      "epoch: 27, train loss: 1.394\n",
      "epoch: 27, train loss: 1.395\n",
      "epoch: 27, train loss: 1.399\n",
      "epoch: 27, train loss: 1.407\n",
      "epoch: 27, train loss: 1.385\n",
      "epoch: 27, train loss: 1.404\n",
      "epoch: 27, train loss: 1.391\n",
      "epoch: 27, train loss: 1.415\n",
      "epoch: 27, train loss: 1.392\n",
      "epoch: 27, train loss: 1.391\n",
      "epoch: 27, train loss: 1.385\n",
      "epoch: 27, train loss: 1.377\n",
      "epoch: 27, train loss: 1.409\n",
      "epoch: 27, train loss: 1.402\n",
      "epoch: 27, train loss: 1.416\n",
      "epoch: 27, train loss: 1.408\n",
      "epoch: 27, train loss: 1.402\n",
      "epoch: 27, train loss: 1.411\n",
      "epoch: 27, train loss: 1.386\n",
      "epoch: 27, train loss: 1.433\n",
      "epoch: 27, train loss: 1.433\n",
      "epoch: 27, train loss: 1.388\n",
      "epoch: 27, train loss: 1.402\n",
      "epoch: 27, train loss: 1.376\n",
      "epoch: 27, train loss: 1.425\n",
      "epoch: 27, train loss: 1.410\n",
      "epoch: 27, train loss: 1.405\n",
      "epoch: 27, train loss: 1.393\n",
      "epoch: 27, train loss: 1.401\n",
      "epoch: 27, train loss: 1.379\n",
      "epoch: 27, train loss: 1.399\n",
      "epoch: 27, train loss: 1.426\n",
      "epoch: 27, train loss: 1.414\n",
      "epoch: 27, train loss: 1.401\n",
      "epoch: 27, train loss: 1.409\n",
      "epoch: 27, train loss: 1.381\n",
      "epoch: 27, train loss: 1.400\n",
      "epoch: 27, train loss: 1.410\n",
      "epoch: 27, train loss: 1.406\n",
      "epoch: 27, train loss: 1.408\n",
      "epoch: 27, train loss: 1.404\n",
      "epoch: 27, train loss: 1.405\n",
      "epoch: 27, train loss: 1.394\n",
      "epoch: 27, train loss: 1.426\n",
      "epoch: 27, train loss: 1.375\n",
      "epoch: 27, train loss: 1.398\n",
      "epoch: 27, train loss: 1.379\n",
      "epoch: 27, train loss: 1.387\n",
      "epoch: 27, train loss: 1.422\n",
      "epoch: 27, train loss: 1.387\n",
      "epoch: 27, train loss: 1.379\n",
      "epoch: 27, train loss: 1.381\n",
      "epoch: 27, train loss: 1.394\n",
      "epoch: 27, train loss: 1.405\n",
      "epoch: 27, train loss: 1.390\n",
      "epoch: 27, train loss: 1.406\n",
      "epoch: 27, train loss: 1.412\n",
      "epoch: 27, train loss: 1.412\n",
      "epoch: 27, train loss: 1.401\n",
      "epoch: 27, train loss: 1.416\n",
      "epoch: 27, train loss: 1.389\n",
      "epoch: 27, train loss: 1.394\n",
      "epoch: 27, train loss: 1.435\n",
      "epoch: 27, train loss: 1.415\n",
      "epoch: 27, train loss: 1.403\n",
      "epoch: 27, train loss: 1.387\n",
      "epoch: 27, train loss: 1.410\n",
      "epoch: 27, train loss: 1.398\n",
      "epoch: 27, train loss: 1.420\n",
      "epoch: 27, train loss: 1.404\n",
      "epoch: 27, train loss: 1.394\n",
      "epoch: 27, train loss: 1.390\n",
      "epoch: 27, train loss: 1.397\n",
      "epoch: 27, train loss: 1.383\n",
      "epoch: 27, train loss: 1.393\n",
      "epoch: 27, train loss: 1.396\n",
      "epoch: 27, train loss: 1.405\n",
      "epoch: 27, train loss: 1.400\n",
      "epoch: 27, train loss: 1.409\n",
      "epoch: 27, train loss: 1.413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b40650f28842b3b7cec83d8496e883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 42000\n",
      "epoch: 27, train loss: 21.005, val loss: 1.316\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a297f2567a457b854053116301624b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28, train loss: 0.088\n",
      "epoch: 28, train loss: 1.385\n",
      "epoch: 28, train loss: 1.432\n",
      "epoch: 28, train loss: 1.436\n",
      "epoch: 28, train loss: 1.414\n",
      "epoch: 28, train loss: 1.389\n",
      "epoch: 28, train loss: 1.394\n",
      "epoch: 28, train loss: 1.405\n",
      "epoch: 28, train loss: 1.364\n",
      "epoch: 28, train loss: 1.380\n",
      "epoch: 28, train loss: 1.397\n",
      "epoch: 28, train loss: 1.385\n",
      "epoch: 28, train loss: 1.402\n",
      "epoch: 28, train loss: 1.391\n",
      "epoch: 28, train loss: 1.411\n",
      "epoch: 28, train loss: 1.394\n",
      "epoch: 28, train loss: 1.380\n",
      "epoch: 28, train loss: 1.415\n",
      "epoch: 28, train loss: 1.406\n",
      "epoch: 28, train loss: 1.424\n",
      "epoch: 28, train loss: 1.403\n",
      "epoch: 28, train loss: 1.398\n",
      "epoch: 28, train loss: 1.393\n",
      "epoch: 28, train loss: 1.372\n",
      "epoch: 28, train loss: 1.390\n",
      "epoch: 28, train loss: 1.374\n",
      "epoch: 28, train loss: 1.405\n",
      "epoch: 28, train loss: 1.400\n",
      "epoch: 28, train loss: 1.392\n",
      "epoch: 28, train loss: 1.400\n",
      "epoch: 28, train loss: 1.412\n",
      "epoch: 28, train loss: 1.407\n",
      "epoch: 28, train loss: 1.390\n",
      "epoch: 28, train loss: 1.386\n",
      "epoch: 28, train loss: 1.388\n",
      "epoch: 28, train loss: 1.392\n",
      "epoch: 28, train loss: 1.411\n",
      "epoch: 28, train loss: 1.385\n",
      "epoch: 28, train loss: 1.387\n",
      "epoch: 28, train loss: 1.397\n",
      "epoch: 28, train loss: 1.357\n",
      "epoch: 28, train loss: 1.394\n",
      "epoch: 28, train loss: 1.383\n",
      "epoch: 28, train loss: 1.378\n",
      "epoch: 28, train loss: 1.390\n",
      "epoch: 28, train loss: 1.387\n",
      "epoch: 28, train loss: 1.402\n",
      "epoch: 28, train loss: 1.393\n",
      "epoch: 28, train loss: 1.387\n",
      "epoch: 28, train loss: 1.383\n",
      "epoch: 28, train loss: 1.369\n",
      "epoch: 28, train loss: 1.394\n",
      "epoch: 28, train loss: 1.404\n",
      "epoch: 28, train loss: 1.399\n",
      "epoch: 28, train loss: 1.412\n",
      "epoch: 28, train loss: 1.391\n",
      "epoch: 28, train loss: 1.375\n",
      "epoch: 28, train loss: 1.395\n",
      "epoch: 28, train loss: 1.386\n",
      "epoch: 28, train loss: 1.388\n",
      "epoch: 28, train loss: 1.398\n",
      "epoch: 28, train loss: 1.395\n",
      "epoch: 28, train loss: 1.384\n",
      "epoch: 28, train loss: 1.394\n",
      "epoch: 28, train loss: 1.378\n",
      "epoch: 28, train loss: 1.392\n",
      "epoch: 28, train loss: 1.406\n",
      "epoch: 28, train loss: 1.408\n",
      "epoch: 28, train loss: 1.382\n",
      "epoch: 28, train loss: 1.383\n",
      "epoch: 28, train loss: 1.391\n",
      "epoch: 28, train loss: 1.381\n",
      "epoch: 28, train loss: 1.376\n",
      "epoch: 28, train loss: 1.382\n",
      "epoch: 28, train loss: 1.393\n",
      "epoch: 28, train loss: 1.387\n",
      "epoch: 28, train loss: 1.380\n",
      "epoch: 28, train loss: 1.377\n",
      "epoch: 28, train loss: 1.397\n",
      "epoch: 28, train loss: 1.394\n",
      "epoch: 28, train loss: 1.379\n",
      "epoch: 28, train loss: 1.413\n",
      "epoch: 28, train loss: 1.387\n",
      "epoch: 28, train loss: 1.394\n",
      "epoch: 28, train loss: 1.398\n",
      "epoch: 28, train loss: 1.397\n",
      "epoch: 28, train loss: 1.377\n",
      "epoch: 28, train loss: 1.376\n",
      "epoch: 28, train loss: 1.374\n",
      "epoch: 28, train loss: 1.363\n",
      "epoch: 28, train loss: 1.385\n",
      "epoch: 28, train loss: 1.374\n",
      "epoch: 28, train loss: 1.400\n",
      "epoch: 28, train loss: 1.389\n",
      "epoch: 28, train loss: 1.411\n",
      "epoch: 28, train loss: 1.386\n",
      "epoch: 28, train loss: 1.393\n",
      "epoch: 28, train loss: 1.381\n",
      "epoch: 28, train loss: 1.391\n",
      "epoch: 28, train loss: 1.386\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fd87741a944f409885c4a50b39fc97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 43500\n",
      "epoch: 28, train loss: 20.963, val loss: 1.312\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92171ff59a914fdcb33245f207b6c26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29, train loss: 0.097\n",
      "epoch: 29, train loss: 1.379\n",
      "epoch: 29, train loss: 1.381\n",
      "epoch: 29, train loss: 1.408\n",
      "epoch: 29, train loss: 1.404\n",
      "epoch: 29, train loss: 1.389\n",
      "epoch: 29, train loss: 1.394\n",
      "epoch: 29, train loss: 1.388\n",
      "epoch: 29, train loss: 1.408\n",
      "epoch: 29, train loss: 1.395\n",
      "epoch: 29, train loss: 1.410\n",
      "epoch: 29, train loss: 1.369\n",
      "epoch: 29, train loss: 1.381\n",
      "epoch: 29, train loss: 1.387\n",
      "epoch: 29, train loss: 1.349\n",
      "epoch: 29, train loss: 1.408\n",
      "epoch: 29, train loss: 1.391\n",
      "epoch: 29, train loss: 1.384\n",
      "epoch: 29, train loss: 1.370\n",
      "epoch: 29, train loss: 1.377\n",
      "epoch: 29, train loss: 1.389\n",
      "epoch: 29, train loss: 1.380\n",
      "epoch: 29, train loss: 1.361\n",
      "epoch: 29, train loss: 1.377\n",
      "epoch: 29, train loss: 1.341\n",
      "epoch: 29, train loss: 1.394\n",
      "epoch: 29, train loss: 1.399\n",
      "epoch: 29, train loss: 1.431\n",
      "epoch: 29, train loss: 1.400\n",
      "epoch: 29, train loss: 1.392\n",
      "epoch: 29, train loss: 1.397\n",
      "epoch: 29, train loss: 1.388\n",
      "epoch: 29, train loss: 1.421\n",
      "epoch: 29, train loss: 1.372\n",
      "epoch: 29, train loss: 1.397\n",
      "epoch: 29, train loss: 1.383\n",
      "epoch: 29, train loss: 1.399\n",
      "epoch: 29, train loss: 1.372\n",
      "epoch: 29, train loss: 1.381\n",
      "epoch: 29, train loss: 1.416\n",
      "epoch: 29, train loss: 1.387\n",
      "epoch: 29, train loss: 1.390\n",
      "epoch: 29, train loss: 1.373\n",
      "epoch: 29, train loss: 1.399\n",
      "epoch: 29, train loss: 1.376\n",
      "epoch: 29, train loss: 1.363\n",
      "epoch: 29, train loss: 1.387\n",
      "epoch: 29, train loss: 1.389\n",
      "epoch: 29, train loss: 1.400\n",
      "epoch: 29, train loss: 1.384\n",
      "epoch: 29, train loss: 1.380\n",
      "epoch: 29, train loss: 1.397\n",
      "epoch: 29, train loss: 1.410\n",
      "epoch: 29, train loss: 1.383\n",
      "epoch: 29, train loss: 1.408\n",
      "epoch: 29, train loss: 1.377\n",
      "epoch: 29, train loss: 1.383\n",
      "epoch: 29, train loss: 1.383\n",
      "epoch: 29, train loss: 1.380\n",
      "epoch: 29, train loss: 1.379\n",
      "epoch: 29, train loss: 1.380\n",
      "epoch: 29, train loss: 1.379\n",
      "epoch: 29, train loss: 1.401\n",
      "epoch: 29, train loss: 1.393\n",
      "epoch: 29, train loss: 1.361\n",
      "epoch: 29, train loss: 1.407\n",
      "epoch: 29, train loss: 1.385\n",
      "epoch: 29, train loss: 1.407\n",
      "epoch: 29, train loss: 1.386\n",
      "epoch: 29, train loss: 1.381\n",
      "epoch: 29, train loss: 1.385\n",
      "epoch: 29, train loss: 1.367\n",
      "epoch: 29, train loss: 1.398\n",
      "epoch: 29, train loss: 1.396\n",
      "epoch: 29, train loss: 1.354\n",
      "epoch: 29, train loss: 1.391\n",
      "epoch: 29, train loss: 1.395\n",
      "epoch: 29, train loss: 1.382\n",
      "epoch: 29, train loss: 1.388\n",
      "epoch: 29, train loss: 1.403\n",
      "epoch: 29, train loss: 1.360\n",
      "epoch: 29, train loss: 1.365\n",
      "epoch: 29, train loss: 1.393\n",
      "epoch: 29, train loss: 1.388\n",
      "epoch: 29, train loss: 1.398\n",
      "epoch: 29, train loss: 1.402\n",
      "epoch: 29, train loss: 1.378\n",
      "epoch: 29, train loss: 1.375\n",
      "epoch: 29, train loss: 1.389\n",
      "epoch: 29, train loss: 1.406\n",
      "epoch: 29, train loss: 1.397\n",
      "epoch: 29, train loss: 1.382\n",
      "epoch: 29, train loss: 1.394\n",
      "epoch: 29, train loss: 1.398\n",
      "epoch: 29, train loss: 1.387\n",
      "epoch: 29, train loss: 1.366\n",
      "epoch: 29, train loss: 1.383\n",
      "epoch: 29, train loss: 1.382\n",
      "epoch: 29, train loss: 1.393\n",
      "epoch: 29, train loss: 1.396\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8fb304f37f475e95bdbfc37ebb4c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 45000\n",
      "epoch: 29, train loss: 20.573, val loss: 1.310\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b607bf9f7f4a26a48c51d0f4c2315d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30, train loss: 0.088\n",
      "epoch: 30, train loss: 1.405\n",
      "epoch: 30, train loss: 1.390\n",
      "epoch: 30, train loss: 1.384\n",
      "epoch: 30, train loss: 1.393\n",
      "epoch: 30, train loss: 1.387\n",
      "epoch: 30, train loss: 1.389\n",
      "epoch: 30, train loss: 1.396\n",
      "epoch: 30, train loss: 1.368\n",
      "epoch: 30, train loss: 1.404\n",
      "epoch: 30, train loss: 1.379\n",
      "epoch: 30, train loss: 1.393\n",
      "epoch: 30, train loss: 1.416\n",
      "epoch: 30, train loss: 1.400\n",
      "epoch: 30, train loss: 1.385\n",
      "epoch: 30, train loss: 1.403\n",
      "epoch: 30, train loss: 1.384\n",
      "epoch: 30, train loss: 1.412\n",
      "epoch: 30, train loss: 1.389\n",
      "epoch: 30, train loss: 1.404\n",
      "epoch: 30, train loss: 1.379\n",
      "epoch: 30, train loss: 1.376\n",
      "epoch: 30, train loss: 1.385\n",
      "epoch: 30, train loss: 1.384\n",
      "epoch: 30, train loss: 1.370\n",
      "epoch: 30, train loss: 1.344\n",
      "epoch: 30, train loss: 1.378\n",
      "epoch: 30, train loss: 1.396\n",
      "epoch: 30, train loss: 1.392\n",
      "epoch: 30, train loss: 1.371\n",
      "epoch: 30, train loss: 1.367\n",
      "epoch: 30, train loss: 1.386\n",
      "epoch: 30, train loss: 1.397\n",
      "epoch: 30, train loss: 1.369\n",
      "epoch: 30, train loss: 1.384\n",
      "epoch: 30, train loss: 1.378\n",
      "epoch: 30, train loss: 1.373\n",
      "epoch: 30, train loss: 1.385\n",
      "epoch: 30, train loss: 1.363\n",
      "epoch: 30, train loss: 1.365\n",
      "epoch: 30, train loss: 1.394\n",
      "epoch: 30, train loss: 1.375\n",
      "epoch: 30, train loss: 1.390\n",
      "epoch: 30, train loss: 1.376\n",
      "epoch: 30, train loss: 1.375\n",
      "epoch: 30, train loss: 1.375\n",
      "epoch: 30, train loss: 1.394\n",
      "epoch: 30, train loss: 1.397\n",
      "epoch: 30, train loss: 1.397\n",
      "epoch: 30, train loss: 1.400\n",
      "epoch: 30, train loss: 1.376\n",
      "epoch: 30, train loss: 1.400\n",
      "epoch: 30, train loss: 1.370\n",
      "epoch: 30, train loss: 1.367\n",
      "epoch: 30, train loss: 1.393\n",
      "epoch: 30, train loss: 1.406\n",
      "epoch: 30, train loss: 1.366\n",
      "epoch: 30, train loss: 1.385\n",
      "epoch: 30, train loss: 1.366\n",
      "epoch: 30, train loss: 1.381\n",
      "epoch: 30, train loss: 1.386\n",
      "epoch: 30, train loss: 1.413\n",
      "epoch: 30, train loss: 1.380\n",
      "epoch: 30, train loss: 1.378\n",
      "epoch: 30, train loss: 1.408\n",
      "epoch: 30, train loss: 1.382\n",
      "epoch: 30, train loss: 1.383\n",
      "epoch: 30, train loss: 1.385\n",
      "epoch: 30, train loss: 1.389\n",
      "epoch: 30, train loss: 1.374\n",
      "epoch: 30, train loss: 1.393\n",
      "epoch: 30, train loss: 1.386\n",
      "epoch: 30, train loss: 1.403\n",
      "epoch: 30, train loss: 1.413\n",
      "epoch: 30, train loss: 1.394\n",
      "epoch: 30, train loss: 1.353\n",
      "epoch: 30, train loss: 1.398\n",
      "epoch: 30, train loss: 1.412\n",
      "epoch: 30, train loss: 1.377\n",
      "epoch: 30, train loss: 1.362\n",
      "epoch: 30, train loss: 1.398\n",
      "epoch: 30, train loss: 1.406\n",
      "epoch: 30, train loss: 1.382\n",
      "epoch: 30, train loss: 1.396\n",
      "epoch: 30, train loss: 1.396\n",
      "epoch: 30, train loss: 1.382\n",
      "epoch: 30, train loss: 1.404\n",
      "epoch: 30, train loss: 1.372\n",
      "epoch: 30, train loss: 1.392\n",
      "epoch: 30, train loss: 1.371\n",
      "epoch: 30, train loss: 1.386\n",
      "epoch: 30, train loss: 1.377\n",
      "epoch: 30, train loss: 1.408\n",
      "epoch: 30, train loss: 1.415\n",
      "epoch: 30, train loss: 1.385\n",
      "epoch: 30, train loss: 1.391\n",
      "epoch: 30, train loss: 1.371\n",
      "epoch: 30, train loss: 1.363\n",
      "epoch: 30, train loss: 1.387\n",
      "epoch: 30, train loss: 1.377\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25bd4d5448374554b930b4b81dc32561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 46500\n",
      "epoch: 30, train loss: 20.905, val loss: 1.309\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be13bb08b0849a092c7a2846f42143e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31, train loss: 0.089\n",
      "epoch: 31, train loss: 1.385\n",
      "epoch: 31, train loss: 1.371\n",
      "epoch: 31, train loss: 1.390\n",
      "epoch: 31, train loss: 1.372\n",
      "epoch: 31, train loss: 1.391\n",
      "epoch: 31, train loss: 1.422\n",
      "epoch: 31, train loss: 1.390\n",
      "epoch: 31, train loss: 1.382\n",
      "epoch: 31, train loss: 1.387\n",
      "epoch: 31, train loss: 1.374\n",
      "epoch: 31, train loss: 1.398\n",
      "epoch: 31, train loss: 1.406\n",
      "epoch: 31, train loss: 1.397\n",
      "epoch: 31, train loss: 1.352\n",
      "epoch: 31, train loss: 1.397\n",
      "epoch: 31, train loss: 1.375\n",
      "epoch: 31, train loss: 1.367\n",
      "epoch: 31, train loss: 1.376\n",
      "epoch: 31, train loss: 1.391\n",
      "epoch: 31, train loss: 1.387\n",
      "epoch: 31, train loss: 1.389\n",
      "epoch: 31, train loss: 1.388\n",
      "epoch: 31, train loss: 1.373\n",
      "epoch: 31, train loss: 1.391\n",
      "epoch: 31, train loss: 1.385\n",
      "epoch: 31, train loss: 1.359\n",
      "epoch: 31, train loss: 1.374\n",
      "epoch: 31, train loss: 1.404\n",
      "epoch: 31, train loss: 1.385\n",
      "epoch: 31, train loss: 1.376\n",
      "epoch: 31, train loss: 1.374\n",
      "epoch: 31, train loss: 1.363\n",
      "epoch: 31, train loss: 1.363\n",
      "epoch: 31, train loss: 1.373\n",
      "epoch: 31, train loss: 1.389\n",
      "epoch: 31, train loss: 1.371\n",
      "epoch: 31, train loss: 1.385\n",
      "epoch: 31, train loss: 1.376\n",
      "epoch: 31, train loss: 1.386\n",
      "epoch: 31, train loss: 1.364\n",
      "epoch: 31, train loss: 1.392\n",
      "epoch: 31, train loss: 1.384\n",
      "epoch: 31, train loss: 1.392\n",
      "epoch: 31, train loss: 1.391\n",
      "epoch: 31, train loss: 1.375\n",
      "epoch: 31, train loss: 1.406\n",
      "epoch: 31, train loss: 1.381\n",
      "epoch: 31, train loss: 1.387\n",
      "epoch: 31, train loss: 1.385\n",
      "epoch: 31, train loss: 1.390\n",
      "epoch: 31, train loss: 1.393\n",
      "epoch: 31, train loss: 1.405\n",
      "epoch: 31, train loss: 1.392\n",
      "epoch: 31, train loss: 1.372\n",
      "epoch: 31, train loss: 1.390\n",
      "epoch: 31, train loss: 1.384\n",
      "epoch: 31, train loss: 1.391\n",
      "epoch: 31, train loss: 1.390\n",
      "epoch: 31, train loss: 1.382\n",
      "epoch: 31, train loss: 1.384\n",
      "epoch: 31, train loss: 1.383\n",
      "epoch: 31, train loss: 1.394\n",
      "epoch: 31, train loss: 1.381\n",
      "epoch: 31, train loss: 1.375\n",
      "epoch: 31, train loss: 1.403\n",
      "epoch: 31, train loss: 1.375\n",
      "epoch: 31, train loss: 1.395\n",
      "epoch: 31, train loss: 1.377\n",
      "epoch: 31, train loss: 1.409\n",
      "epoch: 31, train loss: 1.360\n",
      "epoch: 31, train loss: 1.374\n",
      "epoch: 31, train loss: 1.383\n",
      "epoch: 31, train loss: 1.382\n",
      "epoch: 31, train loss: 1.402\n",
      "epoch: 31, train loss: 1.383\n",
      "epoch: 31, train loss: 1.394\n",
      "epoch: 31, train loss: 1.399\n",
      "epoch: 31, train loss: 1.391\n",
      "epoch: 31, train loss: 1.384\n",
      "epoch: 31, train loss: 1.407\n",
      "epoch: 31, train loss: 1.392\n",
      "epoch: 31, train loss: 1.385\n",
      "epoch: 31, train loss: 1.404\n",
      "epoch: 31, train loss: 1.380\n",
      "epoch: 31, train loss: 1.382\n",
      "epoch: 31, train loss: 1.375\n",
      "epoch: 31, train loss: 1.383\n",
      "epoch: 31, train loss: 1.378\n",
      "epoch: 31, train loss: 1.409\n",
      "epoch: 31, train loss: 1.394\n",
      "epoch: 31, train loss: 1.381\n",
      "epoch: 31, train loss: 1.397\n",
      "epoch: 31, train loss: 1.355\n",
      "epoch: 31, train loss: 1.387\n",
      "epoch: 31, train loss: 1.371\n",
      "epoch: 31, train loss: 1.374\n",
      "epoch: 31, train loss: 1.370\n",
      "epoch: 31, train loss: 1.370\n",
      "epoch: 31, train loss: 1.397\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2e86d34c244ea7866c4385374a8864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint at 48000\n",
      "epoch: 31, train loss: 20.627, val loss: 1.308\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "train(32, model, train_ds.pad_id, optimizer, train_load, val_load, 'cuda', train_ds, scheduler, wandb, 1500, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c085617e",
   "metadata": {
    "cellId": "41egocil31tvcoqmlica4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1dd217f5",
   "metadata": {
    "cellId": "44dp39dvi3ck0kk2q4slm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLAMA(\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (embedding): Embedding(15000, 512)\n",
       "  (transformer): Decoder(\n",
       "    (decoder): ModuleList(\n",
       "      (0-7): 8 x DecoderLayer(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classification): Linear(in_features=512, out_features=15000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "model = LLAMA(8, 512, 8, train_ds.vocab_size, 768, 0.1)\n",
    "check = torch.load('checkpoint.pth', 'cuda')\n",
    "model.load_state_dict(check)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ce6a6",
   "metadata": {
    "cellId": "yf9zgtqxn8fqqd7sny13vk",
    "execution_id": "625ad429-5bc7-4ad6-bf47-0df47e19bd41"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "96c77ddc",
   "metadata": {
    "cellId": "ml4rzsrkcrf5bn08rptt5x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer ok\n",
      "texts ok\n",
      "encoding ok\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "from LLAMA.train import generate\n",
    "\n",
    "ds = TinyStoriesDataset(data_file=\"bpe.vocab\", sp_model_prefix='bpe')\n",
    "\n",
    "def get_text(prefix, max_len=384):\n",
    "    prefix = ds.text2ids(prefix)\n",
    "    prefix = torch.tensor([[ds.bos_id] + prefix])\n",
    "    prefix = prefix.to('cuda')\n",
    "    texts = generate(model, ds.sp_model, 1, ds.pad_id, max_len=max_len, prefix=prefix)\n",
    "    texts = ds.ids2text(texts)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f2561331",
   "metadata": {
    "cellId": "v2dgo35x8nhp3mk9et54vo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tom and lily were playing with their toys in the living room. tom had a red car and lily had a blue doll. they were having fun until they heard a loud noise. they looked around and saw a big hurricane. the sky was dark and the wind was loud.\"wow, look at the hurricane!\" tom said. \"we need to hide inside and there will be monsters and rats.\"\"no, no, no!\" lily said. \"the hurricane is very scary and scary.\"but tom was stubborn and did not listen to lily. he ran to the door of his house and tried to push it. lily followed him, but she slipped. the wind blew the door and it closed behind them. the door closed behind them and lily screamed.\"help, help, help!\" lily cried. tom felt scared and stuck. he ran out of his house and turned around. but it was too late. the hurricane was broken and angry. it made a loud noise. tom and lily were trapped under the sofa and the fabric. they cried and cried. they wished they had listened to lily.the hurricane went away, but it also did not come. tom and lily were very sad and scared. they wished they had listened to lily and made a wish. they never']\n",
      "['lily and tom were twins who liked to play in the park. they had a big yellow barrel that they filled with toys and games. one day, they found a new toy in the park. it was a green thing that made a funny sound when they squeezed it. lily and tom were very happy and excited.\"let\\'s rub this!\" tom said. he found a small stone that was blue and yellow, and a plastic ball. he rubbed the stone and made a funny sound.\"what is this?\" lily asked.\"i don\\'t know. let\\'s keep it. maybe someone lost it,\" tom said.they both rubbed the stone again and again. they made more noises and heard the funny sound. they thought it was too funny to hear.they took turns making funny noises and making sounds. they laughed and had fun. they were very proud of themselves.but then, the sky became dark and loud. tom and lily looked at the sky and saw the cloud getting darker. they felt cold and scared. they shivered and turned around. they wished they had not rubbed the stone.\"stop that, tom. stop that! it\\'s dark and rainy! you\\'re making fire!\" lily said.\"we are sorry, lily. we didn\\'']\n",
      "['emily was very excited. she was going to the park with her friends, which was camping already waiting for the. she asked her mom if she could build a campfire tent. her mom smiled and said, \"that sounds like a really good plan.\"emily nodded, \"i\\'m going to help!\" she was so excited to help.emily put on her coat, and her mom started to design the tent. mom asked, \"are you sure you need some heat?\" emily nodded and proudly said, \"yes! i just need some heat.\"grandma said, \"you can do it, emily. put on some special light and colourful things\". emily put on her helmet and soon the tent was ready. excitedly, emily and her mom built a campfire and decorated it with sunshine and shining bright red flames. they hopped on and danced around the tent. emily was so proud that she had done enough and asked her mom to join them. had found the perfect campfire! looked over the tent in their hot tent and saw bob swimming in the pond. he was splashing and clapping loops. emily was so happy and excited as she shouted \"i look so cool!\" joe smiled and said, \"me too! i set you up on the beach!\" emily ran back']\n",
      "['one day, a little boy named tim found a big, heavy fan in his house. he did not know what the fan was. so, he asked his mom, \"mom, what is that heavy fan?\"his mom said, \"that is a fan, tim. it makes toys safe when it is hot outside. you can play with the fan, but be careful.\"tim played with the fan and it made fun sounds. tim had a lot of fun. but then, something unexpected happened. the fan started to make cold air, and tim got cold.tim tried to cover the fan, but it was too heavy. he felt very sad and cold. his mom saw him and asked, \"what is wrong, tim?\"tim told her about the fan. his mom smiled and said, \"let\\'s try to find out what is making the noise so it keeps warm.\" they looked around and found a small, magic box. they opened it and found that the fan could talk, but inside was fun little juice.tim and his mom laughed and drank their new juice. they told the fan, \"you are very special, and i am not cold anymore.\" from that day on, tim played with the wind whenever it was hot outside.']\n",
      "['once upon a time, there was an octopus named ollie. ollie was a careless octopus. he always left his toys all over the floor. all his friends, max the fish and turtles swam fast to show them how strong he was.one day, max met a little fish named finny. finny wanted to be like ollie, but his mom said, \"you must clean your room first.\" fin needed a big shell to supply his toys. ollie was scared, but he wanted to be a good friend.ollie cleaned his room every day. he made sure his toys were safe from the big ocean. soon, he was not careless anymore. ollie learned that it\\'s good to clean and help his friends. and all his friends along the way, saying hello to their new friend, ollie. and otters learned that being careful is good, even if they are a little fish. and ollie became the best of friends and they all lived happily ever after. felt proud to have helped ollie and finny become a good friend to his mom. and ollie was always happy to have found a friend in fin. the moral of the story is to always be kind and help others, so you don\\'t have to worry to be curious and happy.']\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "prefixes = [\n",
    "    'Tom and Lily were playing with their toys ',\n",
    "    'Lily and Tom were twins who liked to ',\n",
    "    'Emily was very excited. She was going to ',\n",
    "    'One day, a little boy named Tim found a ',\n",
    "    'Once upon a time, there was an octopus named Ollie. Ollie was a '\n",
    "]\n",
    "\n",
    "for p in prefixes:\n",
    "    print(get_text(p), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cf0f60e2",
   "metadata": {
    "cellId": "9pzgw6a733rggv5kb277s7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting evaluate\n",
      "  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting datasets>=2.0.0 (from evaluate)\n",
      "  Obtaining dependency information for datasets>=2.0.0 from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.22.4)\n",
      "Collecting dill (from evaluate)\n",
      "  Obtaining dependency information for dill from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /kernel/fallback/lib/python3.10/site-packages (from evaluate) (2.0.0)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/35/a8/36d8d7b3e46b377800d8dec47891cdf05842d1a2366909ae4a0c89fbc5e6/multiprocess-0.70.15-py310-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
      "Collecting huggingface-hub>=0.7.0 (from evaluate)\n",
      "  Obtaining dependency information for huggingface-hub>=0.7.0 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in /kernel/lib/python3.10/site-packages (from evaluate) (23.2)\n",
      "Collecting responses<0.19 (from evaluate)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets>=2.0.0->evaluate)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /kernel/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.8.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Collecting charset-normalizer~=2.0.0 (from requests>=2.19.0->evaluate)\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /kernel/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /kernel/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /kernel/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /kernel/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Installing collected packages: pyarrow-hotfix, dill, charset-normalizer, multiprocess, responses, huggingface-hub, datasets, evaluate\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script datasets-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script evaluate-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed charset-normalizer-2.0.12 datasets-2.15.0 dill-0.3.7 evaluate-0.4.1 huggingface-hub-0.19.4 multiprocess-0.70.15 pyarrow-hotfix-0.6 responses-0.18.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "%pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "54e5c0b9",
   "metadata": {
    "cellId": "5f7o5gjn64si29ci6391rf"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'maybe_sync' from 'fsspec.asyn' (/usr/local/lib/python3.10/dist-packages/fsspec/asyn.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-042b0c32e4d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"perplexity\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"metric\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mraw_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids2text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/evaluate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevaluation_suite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluationSuite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m from .evaluator import (\n\u001b[1;32m     31\u001b[0m     \u001b[0mAudioClassificationEvaluator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/evaluate/evaluation_suite/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.15.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptimizedTypedSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msanitize_patterns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_reader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnaming\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_split_re\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames_for_dataset_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInMemoryTable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMemoryMappedTable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/download/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdownload_manager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDownloadManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstreaming_download_manager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStreamingDownloadManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilesystems\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOMPRESSION_FILESYSTEMS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m from ..utils.file_utils import (\n\u001b[1;32m     23\u001b[0m     \u001b[0mget_authentication_headers_for_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/filesystems/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_has_s3fs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0ms3filesystem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mS3FileSystem\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m COMPRESSION_FILESYSTEMS: List[compression.BaseCompressedFileFileSystem] = [\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/filesystems/s3filesystem.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0ms3fs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/s3fs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mS3FileSystem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS3File\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mS3Map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/s3fs/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAbstractBufferedFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minfer_storage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masyn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAsyncFileSystem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_sync\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maiobotocore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'maybe_sync' from 'fsspec.asyn' (/usr/local/lib/python3.10/dist-packages/fsspec/asyn.py)"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "import evaluate\n",
    "\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "raw_gen = generate(model, ds.sp_model, 32, ds.pad_id, max_len=256)\n",
    "texts = ds.ids2text(raw_gen)\n",
    "p = perplexity.compute(model_id='gpt2-xl', add_start_token=False, predictions=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af7aa72",
   "metadata": {
    "cellId": "7u9qicrot9eplh6urx8ein"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "cb230409-d96a-453c-bc23-cc7f6194d482",
  "notebookPath": "Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
