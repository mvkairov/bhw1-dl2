{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tinystories import *\n",
    "from llama import LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, loss_fn, pad_idx, device):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    for tgt, length in tqdm(dataloader):\n",
    "        tgt = tgt.to(device)\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        tgt_mask, tgt_padding_mask = create_mask(tgt_input, pad_idx, device)\n",
    "        logits = model(tgt_input, tgt_mask, tgt_padding_mask)\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "    return losses / len(dataloader)\n",
    "\n",
    "\n",
    "def train(n_epochs, model, pad_idx, optimizer, train_loader, val_loader, device, evaluation_step=4000):\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        losses = 0\n",
    "\n",
    "        for i, (tgt, length) in tqdm(enumerate(train_loader)):\n",
    "            tgt = tgt.to(device)\n",
    "            tgt_input = tgt[:-1, :]\n",
    "            tgt_mask, tgt_padding_mask = create_mask(tgt_input, pad_idx, device)\n",
    "            logits = model(tgt_input, tgt_mask, tgt_padding_mask)\n",
    "            optimizer.zero_grad()\n",
    "            tgt_out = tgt[1:, :]\n",
    "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses += loss.item()\n",
    "\n",
    "            if i % evaluation_step == 0:\n",
    "                val_loss = evaluate(model, val_loader, loss_fn, pad_idx, device)\n",
    "                print((f\"Epoch: {epoch}, Train loss: {(losses / evaluation_step):.3f}, Val loss: {val_loss:.3f}\"))\n",
    "                losses = 0\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, loss_fn, pad_idx, device)\n",
    "        print((f\"Epoch: {epoch}, Train loss: {(losses / (len(train_loader) % evaluation_step)):.3f}, Val loss: {val_loss:.3f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tokenizer train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./archive/TinyStoriesV3-GPT4-train.txt\n",
      "  input_format: \n",
      "  model_prefix: \n",
      "  model_type: BPE\n",
      "  vocab_size: 15000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 5\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc_cf\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Internal: /Users/runner/work/sentencepiece/sentencepiece/src/trainer_interface.cc(338) [(output_model_proto_ != nullptr && trainer_spec_.model_prefix().empty()) || (output_model_proto_ == nullptr && !trainer_spec_.model_prefix().empty())] ModelProto and trainer_spec.model_prefix() must be exclusive.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/mvkairov/programming/bhw1-dl2/checkpoint.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mvkairov/programming/bhw1-dl2/checkpoint.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_ds \u001b[39m=\u001b[39m TinyStoriesDataset(data_file\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./archive/TinyStoriesV3-GPT4-train.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, sp_model_prefix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/programming/bhw1-dl2/tinystories.py:31\u001b[0m, in \u001b[0;36mTinyStoriesDataset.__init__\u001b[0;34m(self, data_file, sp_model_prefix, vocab_size, normalization_rule_name, model_type, max_length)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(sp_model_prefix \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.model\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     29\u001b[0m     \u001b[39m# train tokenizer if not trained yet\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStarting tokenizer train...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m     SentencePieceTrainer\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m     32\u001b[0m         \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mdata_file, vocab_size\u001b[39m=\u001b[39;49mvocab_size,\n\u001b[1;32m     33\u001b[0m         model_type\u001b[39m=\u001b[39;49mmodel_type, model_prefix\u001b[39m=\u001b[39;49msp_model_prefix,\n\u001b[1;32m     34\u001b[0m         normalization_rule_name\u001b[39m=\u001b[39;49mnormalization_rule_name, pad_id\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished tokenizer train\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[39m# load tokenizer from file\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dl-hse/lib/python3.11/site-packages/sentencepiece/__init__.py:989\u001b[0m, in \u001b[0;36mSentencePieceTrainer.Train\u001b[0;34m(arg, logstream, **kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    987\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mTrain\u001b[39m(arg\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, logstream\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    988\u001b[0m   \u001b[39mwith\u001b[39;00m _LogStream(ostream\u001b[39m=\u001b[39mlogstream):\n\u001b[0;32m--> 989\u001b[0m     SentencePieceTrainer\u001b[39m.\u001b[39;49m_Train(arg\u001b[39m=\u001b[39;49marg, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/dl-hse/lib/python3.11/site-packages/sentencepiece/__init__.py:982\u001b[0m, in \u001b[0;36mSentencePieceTrainer._Train\u001b[0;34m(arg, **kwargs)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[39mreturn\u001b[39;00m SentencePieceTrainer\u001b[39m.\u001b[39m_TrainFromMap2(new_kwargs, sentence_iterator)\n\u001b[1;32m    981\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 982\u001b[0m     \u001b[39mreturn\u001b[39;00m SentencePieceTrainer\u001b[39m.\u001b[39;49m_TrainFromMap(new_kwargs)\n\u001b[1;32m    984\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dl-hse/lib/python3.11/site-packages/sentencepiece/__init__.py:927\u001b[0m, in \u001b[0;36mSentencePieceTrainer._TrainFromMap\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    926\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_TrainFromMap\u001b[39m(args):\n\u001b[0;32m--> 927\u001b[0m     \u001b[39mreturn\u001b[39;00m _sentencepiece\u001b[39m.\u001b[39;49mSentencePieceTrainer__TrainFromMap(args)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Internal: /Users/runner/work/sentencepiece/sentencepiece/src/trainer_interface.cc(338) [(output_model_proto_ != nullptr && trainer_spec_.model_prefix().empty()) || (output_model_proto_ == nullptr && !trainer_spec_.model_prefix().empty())] ModelProto and trainer_spec.model_prefix() must be exclusive."
     ]
    }
   ],
   "source": [
    "train_ds = TinyStoriesDataset(data_file=\"./archive/TinyStoriesV3-GPT4-train.txt\", sp_model_prefix=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = TinyStoriesDataset(data_file=\"./archive/TinyStoriesV3-GPT4-valid.txt\", sp_model_prefix=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
